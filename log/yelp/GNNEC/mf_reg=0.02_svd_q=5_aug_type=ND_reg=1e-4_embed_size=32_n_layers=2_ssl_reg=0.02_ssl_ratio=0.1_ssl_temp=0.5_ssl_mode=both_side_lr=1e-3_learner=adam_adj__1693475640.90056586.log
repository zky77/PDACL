2023-08-31 17:54:00.900: my pid: 10584
2023-08-31 17:54:00.901: model: model.general_recommender.GNNEC
2023-08-31 17:54:00.901: Dataset statistics:
Name: yelp
The number of users: 7750
The number of items: 28918
The number of ratings: 750318
Average actions of users: 96.82
Average actions of items: 25.95
The sparsity of the dataset: 99.665208%

The number of training: 678579
The number of validation: 0
The number of testing: 71739
2023-08-31 17:54:00.901: NeuRec:[NeuRec]:
recommender=GNNEC
dataset=yelp
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

GNNEC:[hyperparameters]:
mf_reg=0.02
svd_q=5
aug_type=ND
reg=1e-4
embed_size=32
n_layers=2
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
ssl_mode=both_side
lr=1e-3
learner=adam
adj_type=pre
epochs=200
batch_size=4096
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=10
pretrain_flag=0
save_flag=0

Command line:
recommender=GNNEC
dataset=yelp
epochs=200
n_layers=2
embed_size=32
batch_size=4096
lr=1e-3
aug_type=ND
reg=1e-4
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
mf_reg=0.02
svd_q=5
2023-08-31 17:54:20.258: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-08-31 17:54:41.147: [iter 1 : loss : 1.3398 = 0.6214 + 0.3588 + 0.0000 + 0.3595, time: 20.888082]
2023-08-31 17:54:41.879: epoch 1:	0.02541987  	0.05495441  	0.04681705  
2023-08-31 17:54:41.879: Find a better model.
2023-08-31 17:55:02.275: [iter 2 : loss : 1.0702 = 0.4173 + 0.3252 + 0.0001 + 0.3276, time: 20.381361]
2023-08-31 17:55:02.969: epoch 2:	0.02542633  	0.05480605  	0.04684575  
2023-08-31 17:55:23.142: [iter 3 : loss : 0.9762 = 0.3443 + 0.3147 + 0.0002 + 0.3170, time: 20.161372]
2023-08-31 17:55:23.820: epoch 3:	0.02557472  	0.05518572  	0.04695498  
2023-08-31 17:55:23.821: Find a better model.
2023-08-31 17:55:43.991: [iter 4 : loss : 0.9391 = 0.3172 + 0.3096 + 0.0003 + 0.3120, time: 20.159206]
2023-08-31 17:55:44.662: epoch 4:	0.02591667  	0.05586153  	0.04751674  
2023-08-31 17:55:44.662: Find a better model.
2023-08-31 17:56:04.937: [iter 5 : loss : 0.9168 = 0.3021 + 0.3058 + 0.0004 + 0.3085, time: 20.264082]
2023-08-31 17:56:05.583: epoch 5:	0.02644573  	0.05734040  	0.04850833  
2023-08-31 17:56:05.583: Find a better model.
2023-08-31 17:56:25.903: [iter 6 : loss : 0.8987 = 0.2899 + 0.3026 + 0.0004 + 0.3056, time: 20.311836]
2023-08-31 17:56:26.575: epoch 6:	0.02680058  	0.05809548  	0.04899330  
2023-08-31 17:56:26.575: Find a better model.
2023-08-31 17:56:46.674: [iter 7 : loss : 0.8794 = 0.2761 + 0.2998 + 0.0005 + 0.3031, time: 20.087869]
2023-08-31 17:56:47.367: epoch 7:	0.02716834  	0.05901367  	0.04972563  
2023-08-31 17:56:47.367: Find a better model.
2023-08-31 17:57:07.573: [iter 8 : loss : 0.8647 = 0.2660 + 0.2973 + 0.0005 + 0.3009, time: 20.196215]
2023-08-31 17:57:08.278: epoch 8:	0.02759416  	0.05976479  	0.05033553  
2023-08-31 17:57:08.278: Find a better model.
2023-08-31 17:57:28.408: [iter 9 : loss : 0.8519 = 0.2573 + 0.2952 + 0.0006 + 0.2988, time: 20.120208]
2023-08-31 17:57:29.098: epoch 9:	0.02771675  	0.05978934  	0.05042696  
2023-08-31 17:57:29.098: Find a better model.
2023-08-31 17:57:49.381: [iter 10 : loss : 0.8403 = 0.2498 + 0.2931 + 0.0006 + 0.2968, time: 20.273454]
2023-08-31 17:57:50.055: epoch 10:	0.02774900  	0.05968587  	0.05045728  
2023-08-31 17:58:10.277: [iter 11 : loss : 0.8298 = 0.2428 + 0.2913 + 0.0007 + 0.2950, time: 20.212106]
2023-08-31 17:58:10.991: epoch 11:	0.02798772  	0.06023275  	0.05119259  
2023-08-31 17:58:10.992: Find a better model.
2023-08-31 17:58:31.209: [iter 12 : loss : 0.8191 = 0.2358 + 0.2894 + 0.0007 + 0.2932, time: 20.208717]
2023-08-31 17:58:31.912: epoch 12:	0.02832966  	0.06107709  	0.05172904  
2023-08-31 17:58:31.912: Find a better model.
2023-08-31 17:58:52.023: [iter 13 : loss : 0.8105 = 0.2306 + 0.2877 + 0.0008 + 0.2914, time: 20.102482]
2023-08-31 17:58:52.730: epoch 13:	0.02877483  	0.06218201  	0.05224705  
2023-08-31 17:58:52.730: Find a better model.
2023-08-31 17:59:12.974: [iter 14 : loss : 0.7994 = 0.2231 + 0.2859 + 0.0008 + 0.2896, time: 20.228037]
2023-08-31 17:59:13.651: epoch 14:	0.02904581  	0.06290423  	0.05264260  
2023-08-31 17:59:13.651: Find a better model.
2023-08-31 17:59:33.746: [iter 15 : loss : 0.7924 = 0.2196 + 0.2841 + 0.0009 + 0.2878, time: 20.083594]
2023-08-31 17:59:34.440: epoch 15:	0.02938778  	0.06383564  	0.05300057  
2023-08-31 17:59:34.440: Find a better model.
2023-08-31 17:59:54.510: [iter 16 : loss : 0.7847 = 0.2155 + 0.2824 + 0.0009 + 0.2860, time: 20.060700]
2023-08-31 17:59:55.147: epoch 16:	0.02962003  	0.06411678  	0.05336475  
2023-08-31 17:59:55.147: Find a better model.
2023-08-31 18:00:15.328: [iter 17 : loss : 0.7756 = 0.2099 + 0.2805 + 0.0010 + 0.2842, time: 20.172169]
2023-08-31 18:00:16.034: epoch 17:	0.02992974  	0.06477298  	0.05378078  
2023-08-31 18:00:16.034: Find a better model.
2023-08-31 18:00:36.581: [iter 18 : loss : 0.7691 = 0.2070 + 0.2788 + 0.0011 + 0.2823, time: 20.535252]
2023-08-31 18:00:37.297: epoch 18:	0.03026522  	0.06564313  	0.05408404  
2023-08-31 18:00:37.297: Find a better model.
2023-08-31 18:00:57.801: [iter 19 : loss : 0.7603 = 0.2020 + 0.2768 + 0.0011 + 0.2803, time: 20.487332]
2023-08-31 18:00:58.472: epoch 19:	0.03038780  	0.06577691  	0.05446136  
2023-08-31 18:00:58.472: Find a better model.
2023-08-31 18:01:18.631: [iter 20 : loss : 0.7525 = 0.1978 + 0.2750 + 0.0012 + 0.2784, time: 20.144726]
2023-08-31 18:01:19.321: epoch 20:	0.03071687  	0.06653786  	0.05493379  
2023-08-31 18:01:19.321: Find a better model.
2023-08-31 18:01:39.436: [iter 21 : loss : 0.7437 = 0.1929 + 0.2731 + 0.0012 + 0.2764, time: 20.102815]
2023-08-31 18:01:40.101: epoch 21:	0.03119429  	0.06765612  	0.05566069  
2023-08-31 18:01:40.101: Find a better model.
2023-08-31 18:02:00.096: [iter 22 : loss : 0.7365 = 0.1896 + 0.2712 + 0.0013 + 0.2744, time: 19.982730]
2023-08-31 18:02:00.800: epoch 22:	0.03153624  	0.06835961  	0.05646394  
2023-08-31 18:02:00.800: Find a better model.
2023-08-31 18:02:21.138: [iter 23 : loss : 0.7286 = 0.1857 + 0.2692 + 0.0014 + 0.2723, time: 20.319964]
2023-08-31 18:02:21.827: epoch 23:	0.03196852  	0.06937724  	0.05708417  
2023-08-31 18:02:21.827: Find a better model.
2023-08-31 18:02:42.062: [iter 24 : loss : 0.7223 = 0.1834 + 0.2673 + 0.0014 + 0.2703, time: 20.224105]
2023-08-31 18:02:42.747: epoch 24:	0.03211044  	0.06945495  	0.05731634  
2023-08-31 18:02:42.747: Find a better model.
2023-08-31 18:03:02.972: [iter 25 : loss : 0.7147 = 0.1797 + 0.2653 + 0.0015 + 0.2682, time: 20.212624]
2023-08-31 18:03:03.661: epoch 25:	0.03248464  	0.07046919  	0.05814448  
2023-08-31 18:03:03.661: Find a better model.
2023-08-31 18:03:23.865: [iter 26 : loss : 0.7077 = 0.1768 + 0.2633 + 0.0016 + 0.2661, time: 20.192895]
2023-08-31 18:03:24.561: epoch 26:	0.03278143  	0.07093499  	0.05869278  
2023-08-31 18:03:24.561: Find a better model.
2023-08-31 18:03:44.865: [iter 27 : loss : 0.6996 = 0.1729 + 0.2612 + 0.0017 + 0.2639, time: 20.291843]
2023-08-31 18:03:45.543: epoch 27:	0.03304595  	0.07154164  	0.05933160  
2023-08-31 18:03:45.543: Find a better model.
2023-08-31 18:04:05.777: [iter 28 : loss : 0.6928 = 0.1703 + 0.2591 + 0.0017 + 0.2617, time: 20.221258]
2023-08-31 18:04:06.480: epoch 28:	0.03338137  	0.07228887  	0.05997109  
2023-08-31 18:04:06.480: Find a better model.
2023-08-31 18:04:26.664: [iter 29 : loss : 0.6869 = 0.1686 + 0.2570 + 0.0018 + 0.2595, time: 20.166987]
2023-08-31 18:04:27.336: epoch 29:	0.03356202  	0.07254133  	0.06033628  
2023-08-31 18:04:27.336: Find a better model.
2023-08-31 18:04:47.264: [iter 30 : loss : 0.6796 = 0.1653 + 0.2551 + 0.0019 + 0.2574, time: 19.918209]
2023-08-31 18:04:47.961: epoch 30:	0.03384585  	0.07327055  	0.06117540  
2023-08-31 18:04:47.961: Find a better model.
2023-08-31 18:05:08.180: [iter 31 : loss : 0.6723 = 0.1623 + 0.2529 + 0.0020 + 0.2551, time: 20.208564]
2023-08-31 18:05:08.889: epoch 31:	0.03416196  	0.07406227  	0.06178846  
2023-08-31 18:05:08.889: Find a better model.
2023-08-31 18:05:29.250: [iter 32 : loss : 0.6647 = 0.1591 + 0.2507 + 0.0020 + 0.2529, time: 20.352259]
2023-08-31 18:05:29.943: epoch 32:	0.03472319  	0.07513519  	0.06266636  
2023-08-31 18:05:29.943: Find a better model.
2023-08-31 18:05:50.314: [iter 33 : loss : 0.6585 = 0.1572 + 0.2485 + 0.0021 + 0.2507, time: 20.358183]
2023-08-31 18:05:51.001: epoch 33:	0.03507798  	0.07587001  	0.06339683  
2023-08-31 18:05:51.001: Find a better model.
2023-08-31 18:06:11.409: [iter 34 : loss : 0.6525 = 0.1555 + 0.2463 + 0.0022 + 0.2484, time: 20.395898]
2023-08-31 18:06:12.096: epoch 34:	0.03542634  	0.07656885  	0.06406981  
2023-08-31 18:06:12.096: Find a better model.
2023-08-31 18:06:32.260: [iter 35 : loss : 0.6460 = 0.1532 + 0.2443 + 0.0023 + 0.2462, time: 20.153502]
2023-08-31 18:06:32.926: epoch 35:	0.03576826  	0.07737041  	0.06455785  
2023-08-31 18:06:32.927: Find a better model.
2023-08-31 18:06:53.066: [iter 36 : loss : 0.6403 = 0.1518 + 0.2421 + 0.0024 + 0.2441, time: 20.130093]
2023-08-31 18:06:53.699: epoch 36:	0.03599405  	0.07789030  	0.06518579  
2023-08-31 18:06:53.699: Find a better model.
2023-08-31 18:07:13.819: [iter 37 : loss : 0.6332 = 0.1488 + 0.2401 + 0.0025 + 0.2419, time: 20.111036]
2023-08-31 18:07:14.486: epoch 37:	0.03616822  	0.07839176  	0.06566916  
2023-08-31 18:07:14.487: Find a better model.
2023-08-31 18:07:34.454: [iter 38 : loss : 0.6269 = 0.1463 + 0.2382 + 0.0025 + 0.2398, time: 19.957206]
2023-08-31 18:07:35.109: epoch 38:	0.03663912  	0.07934794  	0.06635620  
2023-08-31 18:07:35.109: Find a better model.
2023-08-31 18:07:55.210: [iter 39 : loss : 0.6212 = 0.1450 + 0.2360 + 0.0026 + 0.2376, time: 20.091381]
2023-08-31 18:07:55.900: epoch 39:	0.03703912  	0.08036106  	0.06720974  
2023-08-31 18:07:55.900: Find a better model.
2023-08-31 18:08:15.899: [iter 40 : loss : 0.6151 = 0.1430 + 0.2339 + 0.0027 + 0.2355, time: 19.988950]
2023-08-31 18:08:16.581: epoch 40:	0.03734876  	0.08107936  	0.06774624  
2023-08-31 18:08:16.582: Find a better model.
2023-08-31 18:08:36.803: [iter 41 : loss : 0.6085 = 0.1405 + 0.2319 + 0.0028 + 0.2334, time: 20.212753]
2023-08-31 18:08:37.472: epoch 41:	0.03745841  	0.08126047  	0.06806489  
2023-08-31 18:08:37.472: Find a better model.
2023-08-31 18:08:57.650: [iter 42 : loss : 0.6031 = 0.1390 + 0.2299 + 0.0029 + 0.2313, time: 20.170574]
2023-08-31 18:08:58.339: epoch 42:	0.03769066  	0.08190618  	0.06852347  
2023-08-31 18:08:58.340: Find a better model.
2023-08-31 18:09:18.539: [iter 43 : loss : 0.5971 = 0.1370 + 0.2278 + 0.0030 + 0.2293, time: 20.188024]
2023-08-31 18:09:19.202: epoch 43:	0.03793582  	0.08239597  	0.06900374  
2023-08-31 18:09:19.202: Find a better model.
2023-08-31 18:09:39.257: [iter 44 : loss : 0.5927 = 0.1366 + 0.2258 + 0.0031 + 0.2272, time: 20.045907]
2023-08-31 18:09:39.956: epoch 44:	0.03820030  	0.08303522  	0.06953228  
2023-08-31 18:09:39.956: Find a better model.
2023-08-31 18:10:00.282: [iter 45 : loss : 0.5874 = 0.1351 + 0.2239 + 0.0032 + 0.2252, time: 20.316675]
2023-08-31 18:10:00.956: epoch 45:	0.03850348  	0.08366831  	0.06993858  
2023-08-31 18:10:00.957: Find a better model.
2023-08-31 18:10:21.125: [iter 46 : loss : 0.5827 = 0.1343 + 0.2219 + 0.0033 + 0.2232, time: 20.155985]
2023-08-31 18:10:21.777: epoch 46:	0.03876797  	0.08423692  	0.07047765  
2023-08-31 18:10:21.777: Find a better model.
2023-08-31 18:10:42.003: [iter 47 : loss : 0.5763 = 0.1318 + 0.2199 + 0.0034 + 0.2212, time: 20.215479]
2023-08-31 18:10:42.690: epoch 47:	0.03892281  	0.08449579  	0.07076042  
2023-08-31 18:10:42.690: Find a better model.
2023-08-31 18:11:02.835: [iter 48 : loss : 0.5716 = 0.1307 + 0.2181 + 0.0035 + 0.2193, time: 20.131126]
2023-08-31 18:11:03.531: epoch 48:	0.03934857  	0.08536264  	0.07135049  
2023-08-31 18:11:03.531: Find a better model.
2023-08-31 18:11:23.603: [iter 49 : loss : 0.5668 = 0.1297 + 0.2161 + 0.0036 + 0.2174, time: 20.060590]
2023-08-31 18:11:24.257: epoch 49:	0.03951630  	0.08578122  	0.07193644  
2023-08-31 18:11:24.257: Find a better model.
2023-08-31 18:11:44.309: [iter 50 : loss : 0.5627 = 0.1291 + 0.2144 + 0.0037 + 0.2155, time: 20.036865]
2023-08-31 18:11:44.973: epoch 50:	0.03972920  	0.08647693  	0.07233892  
2023-08-31 18:11:44.974: Find a better model.
2023-08-31 18:12:05.074: [iter 51 : loss : 0.5579 = 0.1279 + 0.2126 + 0.0038 + 0.2137, time: 20.090722]
2023-08-31 18:12:05.767: epoch 51:	0.03973562  	0.08652912  	0.07249485  
2023-08-31 18:12:05.768: Find a better model.
2023-08-31 18:12:26.015: [iter 52 : loss : 0.5531 = 0.1266 + 0.2108 + 0.0039 + 0.2118, time: 20.236066]
2023-08-31 18:12:26.691: epoch 52:	0.03982595  	0.08683039  	0.07264189  
2023-08-31 18:12:26.691: Find a better model.
2023-08-31 18:12:47.042: [iter 53 : loss : 0.5490 = 0.1262 + 0.2089 + 0.0040 + 0.2100, time: 20.339190]
2023-08-31 18:12:47.731: epoch 53:	0.03999367  	0.08701987  	0.07284590  
2023-08-31 18:12:47.731: Find a better model.
2023-08-31 18:13:07.930: [iter 54 : loss : 0.5438 = 0.1242 + 0.2073 + 0.0040 + 0.2082, time: 20.185949]
2023-08-31 18:13:08.606: epoch 54:	0.04033556  	0.08772655  	0.07347459  
2023-08-31 18:13:08.606: Find a better model.
2023-08-31 18:13:28.786: [iter 55 : loss : 0.5398 = 0.1239 + 0.2054 + 0.0041 + 0.2064, time: 20.171761]
2023-08-31 18:13:29.484: epoch 55:	0.04042586  	0.08781605  	0.07355466  
2023-08-31 18:13:29.485: Find a better model.
2023-08-31 18:13:49.845: [iter 56 : loss : 0.5356 = 0.1230 + 0.2037 + 0.0042 + 0.2046, time: 20.351235]
2023-08-31 18:13:50.568: epoch 56:	0.04069683  	0.08837589  	0.07395720  
2023-08-31 18:13:50.569: Find a better model.
2023-08-31 18:14:10.968: [iter 57 : loss : 0.5314 = 0.1221 + 0.2020 + 0.0043 + 0.2030, time: 20.388471]
2023-08-31 18:14:11.674: epoch 57:	0.04079358  	0.08855984  	0.07408120  
2023-08-31 18:14:11.674: Find a better model.
2023-08-31 18:14:31.878: [iter 58 : loss : 0.5292 = 0.1230 + 0.2005 + 0.0044 + 0.2013, time: 20.194144]
2023-08-31 18:14:32.584: epoch 58:	0.04094844  	0.08900602  	0.07436836  
2023-08-31 18:14:32.584: Find a better model.
2023-08-31 18:14:52.785: [iter 59 : loss : 0.5242 = 0.1214 + 0.1986 + 0.0045 + 0.1996, time: 20.191881]
2023-08-31 18:14:53.452: epoch 59:	0.04109034  	0.08918574  	0.07461450  
2023-08-31 18:14:53.452: Find a better model.
2023-08-31 18:15:13.768: [iter 60 : loss : 0.5199 = 0.1200 + 0.1972 + 0.0046 + 0.1980, time: 20.302908]
2023-08-31 18:15:14.429: epoch 60:	0.04134836  	0.08984306  	0.07531123  
2023-08-31 18:15:14.429: Find a better model.
2023-08-31 18:15:34.620: [iter 61 : loss : 0.5157 = 0.1192 + 0.1955 + 0.0047 + 0.1963, time: 20.180212]
2023-08-31 18:15:35.317: epoch 61:	0.04132898  	0.08990891  	0.07542831  
2023-08-31 18:15:35.318: Find a better model.
2023-08-31 18:15:55.411: [iter 62 : loss : 0.5125 = 0.1191 + 0.1938 + 0.0048 + 0.1947, time: 20.081321]
2023-08-31 18:15:56.114: epoch 62:	0.04124513  	0.08981212  	0.07545713  
2023-08-31 18:16:16.274: [iter 63 : loss : 0.5074 = 0.1169 + 0.1924 + 0.0049 + 0.1932, time: 20.147655]
2023-08-31 18:16:16.946: epoch 63:	0.04143219  	0.09019867  	0.07590313  
2023-08-31 18:16:16.947: Find a better model.
2023-08-31 18:16:37.323: [iter 64 : loss : 0.5039 = 0.1165 + 0.1909 + 0.0050 + 0.1916, time: 20.364699]
2023-08-31 18:16:38.030: epoch 64:	0.04145156  	0.09026396  	0.07599803  
2023-08-31 18:16:38.030: Find a better model.
2023-08-31 18:16:58.409: [iter 65 : loss : 0.5018 = 0.1174 + 0.1893 + 0.0051 + 0.1900, time: 20.365210]
2023-08-31 18:16:59.084: epoch 65:	0.04166445  	0.09072135  	0.07631056  
2023-08-31 18:16:59.084: Find a better model.
2023-08-31 18:17:19.311: [iter 66 : loss : 0.4963 = 0.1148 + 0.1878 + 0.0052 + 0.1885, time: 20.215638]
2023-08-31 18:17:19.967: epoch 66:	0.04174833  	0.09093456  	0.07650332  
2023-08-31 18:17:19.967: Find a better model.
2023-08-31 18:17:39.999: [iter 67 : loss : 0.4932 = 0.1146 + 0.1863 + 0.0053 + 0.1870, time: 20.021157]
2023-08-31 18:17:40.677: epoch 67:	0.04190316  	0.09107981  	0.07676776  
2023-08-31 18:17:40.677: Find a better model.
2023-08-31 18:18:00.739: [iter 68 : loss : 0.4910 = 0.1152 + 0.1849 + 0.0054 + 0.1855, time: 20.050845]
2023-08-31 18:18:01.393: epoch 68:	0.04198057  	0.09143995  	0.07706727  
2023-08-31 18:18:01.393: Find a better model.
2023-08-31 18:18:21.537: [iter 69 : loss : 0.4882 = 0.1152 + 0.1835 + 0.0055 + 0.1841, time: 20.130751]
2023-08-31 18:18:22.252: epoch 69:	0.04185802  	0.09103462  	0.07703628  
2023-08-31 18:18:42.628: [iter 70 : loss : 0.4848 = 0.1144 + 0.1822 + 0.0056 + 0.1827, time: 20.361483]
2023-08-31 18:18:43.299: epoch 70:	0.04203219  	0.09154567  	0.07721861  
2023-08-31 18:18:43.299: Find a better model.
2023-08-31 18:19:03.404: [iter 71 : loss : 0.4803 = 0.1127 + 0.1807 + 0.0057 + 0.1812, time: 20.092570]
2023-08-31 18:19:04.071: epoch 71:	0.04203865  	0.09162754  	0.07722545  
2023-08-31 18:19:04.071: Find a better model.
2023-08-31 18:19:24.325: [iter 72 : loss : 0.4778 = 0.1130 + 0.1792 + 0.0058 + 0.1799, time: 20.244886]
2023-08-31 18:19:25.018: epoch 72:	0.04205799  	0.09174180  	0.07738783  
2023-08-31 18:19:25.018: Find a better model.
2023-08-31 18:19:45.356: [iter 73 : loss : 0.4741 = 0.1117 + 0.1780 + 0.0059 + 0.1785, time: 20.327355]
2023-08-31 18:19:46.082: epoch 73:	0.04221928  	0.09212669  	0.07770711  
2023-08-31 18:19:46.082: Find a better model.
2023-08-31 18:20:06.378: [iter 74 : loss : 0.4704 = 0.1108 + 0.1765 + 0.0060 + 0.1771, time: 20.284448]
2023-08-31 18:20:07.072: epoch 74:	0.04234830  	0.09230865  	0.07787386  
2023-08-31 18:20:07.073: Find a better model.
2023-08-31 18:20:27.495: [iter 75 : loss : 0.4678 = 0.1107 + 0.1752 + 0.0061 + 0.1758, time: 20.409238]
2023-08-31 18:20:28.176: epoch 75:	0.04238702  	0.09235575  	0.07805317  
2023-08-31 18:20:28.176: Find a better model.
2023-08-31 18:20:48.645: [iter 76 : loss : 0.4661 = 0.1113 + 0.1741 + 0.0062 + 0.1745, time: 20.456283]
2023-08-31 18:20:49.341: epoch 76:	0.04249666  	0.09266251  	0.07814142  
2023-08-31 18:20:49.341: Find a better model.
2023-08-31 18:21:09.562: [iter 77 : loss : 0.4621 = 0.1097 + 0.1729 + 0.0063 + 0.1732, time: 20.211822]
2023-08-31 18:21:10.246: epoch 77:	0.04249664  	0.09276875  	0.07818396  
2023-08-31 18:21:10.246: Find a better model.
2023-08-31 18:21:30.675: [iter 78 : loss : 0.4599 = 0.1100 + 0.1716 + 0.0064 + 0.1720, time: 20.415972]
2023-08-31 18:21:31.346: epoch 78:	0.04258049  	0.09307375  	0.07838853  
2023-08-31 18:21:31.346: Find a better model.
2023-08-31 18:21:51.576: [iter 79 : loss : 0.4572 = 0.1096 + 0.1704 + 0.0065 + 0.1708, time: 20.218190]
2023-08-31 18:21:52.249: epoch 79:	0.04269660  	0.09328053  	0.07859435  
2023-08-31 18:21:52.249: Find a better model.
2023-08-31 18:22:12.673: [iter 80 : loss : 0.4544 = 0.1091 + 0.1692 + 0.0066 + 0.1696, time: 20.412367]
2023-08-31 18:22:13.346: epoch 80:	0.04273532  	0.09318547  	0.07879926  
2023-08-31 18:22:33.606: [iter 81 : loss : 0.4512 = 0.1084 + 0.1678 + 0.0067 + 0.1684, time: 20.250385]
2023-08-31 18:22:34.308: epoch 81:	0.04287724  	0.09363849  	0.07890279  
2023-08-31 18:22:34.309: Find a better model.
2023-08-31 18:22:54.500: [iter 82 : loss : 0.4488 = 0.1081 + 0.1667 + 0.0068 + 0.1672, time: 20.178169]
2023-08-31 18:22:55.195: epoch 82:	0.04297404  	0.09379639  	0.07896627  
2023-08-31 18:22:55.195: Find a better model.
2023-08-31 18:23:15.241: [iter 83 : loss : 0.4459 = 0.1074 + 0.1656 + 0.0069 + 0.1660, time: 20.036265]
2023-08-31 18:23:15.916: epoch 83:	0.04292887  	0.09359425  	0.07897098  
2023-08-31 18:23:36.313: [iter 84 : loss : 0.4428 = 0.1064 + 0.1646 + 0.0070 + 0.1649, time: 20.382488]
2023-08-31 18:23:37.015: epoch 84:	0.04301918  	0.09387802  	0.07922297  
2023-08-31 18:23:37.015: Find a better model.
2023-08-31 18:23:57.235: [iter 85 : loss : 0.4417 = 0.1074 + 0.1634 + 0.0070 + 0.1638, time: 20.206818]
2023-08-31 18:23:57.919: epoch 85:	0.04319337  	0.09435299  	0.07945554  
2023-08-31 18:23:57.919: Find a better model.
2023-08-31 18:24:18.268: [iter 86 : loss : 0.4381 = 0.1058 + 0.1624 + 0.0071 + 0.1628, time: 20.336573]
2023-08-31 18:24:18.930: epoch 86:	0.04321274  	0.09432855  	0.07961412  
2023-08-31 18:24:39.367: [iter 87 : loss : 0.4368 = 0.1065 + 0.1614 + 0.0072 + 0.1617, time: 20.420670]
2023-08-31 18:24:40.042: epoch 87:	0.04332888  	0.09459244  	0.07975694  
2023-08-31 18:24:40.042: Find a better model.
2023-08-31 18:25:00.184: [iter 88 : loss : 0.4347 = 0.1065 + 0.1602 + 0.0073 + 0.1607, time: 20.131406]
2023-08-31 18:25:00.841: epoch 88:	0.04330307  	0.09456717  	0.07985080  
2023-08-31 18:25:21.019: [iter 89 : loss : 0.4320 = 0.1054 + 0.1594 + 0.0074 + 0.1597, time: 20.167353]
2023-08-31 18:25:21.700: epoch 89:	0.04323211  	0.09455155  	0.07985702  
2023-08-31 18:25:42.093: [iter 90 : loss : 0.4296 = 0.1050 + 0.1584 + 0.0075 + 0.1587, time: 20.380970]
2023-08-31 18:25:42.761: epoch 90:	0.04324500  	0.09453315  	0.07994952  
2023-08-31 18:26:03.125: [iter 91 : loss : 0.4291 = 0.1065 + 0.1574 + 0.0076 + 0.1577, time: 20.350709]
2023-08-31 18:26:03.842: epoch 91:	0.04340627  	0.09483401  	0.08006459  
2023-08-31 18:26:03.842: Find a better model.
2023-08-31 18:26:24.119: [iter 92 : loss : 0.4259 = 0.1050 + 0.1565 + 0.0077 + 0.1567, time: 20.263227]
2023-08-31 18:26:24.795: epoch 92:	0.04327725  	0.09459126  	0.07994654  
2023-08-31 18:26:44.896: [iter 93 : loss : 0.4232 = 0.1041 + 0.1556 + 0.0078 + 0.1558, time: 20.086405]
2023-08-31 18:26:45.551: epoch 93:	0.04316757  	0.09429166  	0.07971037  
2023-08-31 18:27:05.569: [iter 94 : loss : 0.4232 = 0.1059 + 0.1546 + 0.0079 + 0.1548, time: 20.009528]
2023-08-31 18:27:06.220: epoch 94:	0.04325790  	0.09447358  	0.07991312  
2023-08-31 18:27:26.619: [iter 95 : loss : 0.4200 = 0.1045 + 0.1536 + 0.0079 + 0.1540, time: 20.387435]
2023-08-31 18:27:27.286: epoch 95:	0.04336113  	0.09484888  	0.08004169  
2023-08-31 18:27:27.287: Find a better model.
2023-08-31 18:27:47.255: [iter 96 : loss : 0.4196 = 0.1056 + 0.1529 + 0.0080 + 0.1531, time: 19.956352]
2023-08-31 18:27:47.951: epoch 96:	0.04329016  	0.09461548  	0.08002304  
2023-08-31 18:28:08.295: [iter 97 : loss : 0.4174 = 0.1049 + 0.1521 + 0.0081 + 0.1523, time: 20.333144]
2023-08-31 18:28:09.010: epoch 97:	0.04347079  	0.09512851  	0.08015720  
2023-08-31 18:28:09.010: Find a better model.
2023-08-31 18:28:29.482: [iter 98 : loss : 0.4144 = 0.1037 + 0.1511 + 0.0082 + 0.1514, time: 20.459853]
2023-08-31 18:28:30.157: epoch 98:	0.04350953  	0.09517272  	0.08024705  
2023-08-31 18:28:30.157: Find a better model.
2023-08-31 18:28:50.416: [iter 99 : loss : 0.4119 = 0.1026 + 0.1504 + 0.0083 + 0.1506, time: 20.249856]
2023-08-31 18:28:51.112: epoch 99:	0.04349018  	0.09508333  	0.08021785  
2023-08-31 18:29:11.429: [iter 100 : loss : 0.4107 = 0.1031 + 0.1495 + 0.0084 + 0.1497, time: 20.305640]
2023-08-31 18:29:12.114: epoch 100:	0.04341275  	0.09492841  	0.08025832  
2023-08-31 18:29:32.426: [iter 101 : loss : 0.4079 = 0.1018 + 0.1487 + 0.0085 + 0.1490, time: 20.302329]
2023-08-31 18:29:33.100: epoch 101:	0.04330951  	0.09468423  	0.08015107  
2023-08-31 18:29:53.390: [iter 102 : loss : 0.4092 = 0.1046 + 0.1479 + 0.0085 + 0.1482, time: 20.278780]
2023-08-31 18:29:54.077: epoch 102:	0.04343208  	0.09490544  	0.08037419  
2023-08-31 18:30:14.232: [iter 103 : loss : 0.4068 = 0.1036 + 0.1472 + 0.0086 + 0.1475, time: 20.143190]
2023-08-31 18:30:14.934: epoch 103:	0.04333530  	0.09467546  	0.08023760  
2023-08-31 18:30:35.300: [iter 104 : loss : 0.4060 = 0.1040 + 0.1466 + 0.0087 + 0.1467, time: 20.350054]
2023-08-31 18:30:35.976: epoch 104:	0.04353528  	0.09506955  	0.08046823  
2023-08-31 18:30:56.377: [iter 105 : loss : 0.4022 = 0.1016 + 0.1458 + 0.0088 + 0.1460, time: 20.389601]
2023-08-31 18:30:57.059: epoch 105:	0.04357399  	0.09512721  	0.08041731  
2023-08-31 18:31:17.270: [iter 106 : loss : 0.3990 = 0.0997 + 0.1451 + 0.0089 + 0.1453, time: 20.201689]
2023-08-31 18:31:17.908: epoch 106:	0.04353530  	0.09496153  	0.08038785  
2023-08-31 18:31:38.218: [iter 107 : loss : 0.3992 = 0.1014 + 0.1444 + 0.0089 + 0.1446, time: 20.300864]
2023-08-31 18:31:38.897: epoch 107:	0.04346435  	0.09488496  	0.08040894  
2023-08-31 18:31:59.186: [iter 108 : loss : 0.3986 = 0.1019 + 0.1438 + 0.0090 + 0.1439, time: 20.275855]
2023-08-31 18:31:59.883: epoch 108:	0.04354821  	0.09518241  	0.08060084  
2023-08-31 18:31:59.883: Find a better model.
2023-08-31 18:32:20.207: [iter 109 : loss : 0.3962 = 0.1010 + 0.1430 + 0.0091 + 0.1432, time: 20.311672]
2023-08-31 18:32:20.902: epoch 109:	0.04359982  	0.09516165  	0.08060388  
2023-08-31 18:32:41.293: [iter 110 : loss : 0.3939 = 0.0998 + 0.1424 + 0.0092 + 0.1425, time: 20.376271]
2023-08-31 18:32:41.965: epoch 110:	0.04359981  	0.09516276  	0.08056131  
2023-08-31 18:33:02.282: [iter 111 : loss : 0.3925 = 0.0996 + 0.1418 + 0.0092 + 0.1419, time: 20.308457]
2023-08-31 18:33:02.947: epoch 111:	0.04368367  	0.09532557  	0.08069788  
2023-08-31 18:33:02.947: Find a better model.
2023-08-31 18:33:23.279: [iter 112 : loss : 0.3941 = 0.1022 + 0.1413 + 0.0093 + 0.1413, time: 20.319850]
2023-08-31 18:33:23.946: epoch 112:	0.04377401  	0.09554705  	0.08090242  
2023-08-31 18:33:23.947: Find a better model.
2023-08-31 18:33:44.239: [iter 113 : loss : 0.3927 = 0.1020 + 0.1406 + 0.0094 + 0.1407, time: 20.282416]
2023-08-31 18:33:44.938: epoch 113:	0.04374818  	0.09567359  	0.08081803  
2023-08-31 18:33:44.938: Find a better model.
2023-08-31 18:34:05.325: [iter 114 : loss : 0.3890 = 0.0994 + 0.1400 + 0.0095 + 0.1401, time: 20.372545]
2023-08-31 18:34:06.002: epoch 114:	0.04387074  	0.09585331  	0.08086216  
2023-08-31 18:34:06.002: Find a better model.
2023-08-31 18:34:26.340: [iter 115 : loss : 0.3889 = 0.1005 + 0.1393 + 0.0095 + 0.1395, time: 20.327477]
2023-08-31 18:34:27.031: epoch 115:	0.04383851  	0.09583183  	0.08088223  
2023-08-31 18:34:47.090: [iter 116 : loss : 0.3866 = 0.0993 + 0.1388 + 0.0096 + 0.1390, time: 20.046205]
2023-08-31 18:34:47.775: epoch 116:	0.04397397  	0.09595342  	0.08104405  
2023-08-31 18:34:47.775: Find a better model.
2023-08-31 18:35:08.128: [iter 117 : loss : 0.3864 = 0.1001 + 0.1382 + 0.0097 + 0.1384, time: 20.337906]
2023-08-31 18:35:08.825: epoch 117:	0.04395460  	0.09595663  	0.08117680  
2023-08-31 18:35:08.825: Find a better model.
2023-08-31 18:35:29.032: [iter 118 : loss : 0.3855 = 0.1000 + 0.1378 + 0.0098 + 0.1379, time: 20.197391]
2023-08-31 18:35:29.703: epoch 118:	0.04398041  	0.09608761  	0.08117657  
2023-08-31 18:35:29.703: Find a better model.
2023-08-31 18:35:49.973: [iter 119 : loss : 0.3838 = 0.0996 + 0.1371 + 0.0098 + 0.1373, time: 20.258913]
2023-08-31 18:35:50.660: epoch 119:	0.04402555  	0.09611113  	0.08123430  
2023-08-31 18:35:50.660: Find a better model.
2023-08-31 18:36:10.933: [iter 120 : loss : 0.3832 = 0.1000 + 0.1366 + 0.0099 + 0.1367, time: 20.253977]
2023-08-31 18:36:11.605: epoch 120:	0.04389009  	0.09572998  	0.08105461  
2023-08-31 18:36:31.950: [iter 121 : loss : 0.3826 = 0.1001 + 0.1362 + 0.0100 + 0.1363, time: 20.331434]
2023-08-31 18:36:32.629: epoch 121:	0.04394172  	0.09602717  	0.08118039  
2023-08-31 18:36:53.009: [iter 122 : loss : 0.3792 = 0.0979 + 0.1356 + 0.0100 + 0.1357, time: 20.363569]
2023-08-31 18:36:53.707: epoch 122:	0.04389657  	0.09576339  	0.08106162  
2023-08-31 18:37:14.250: [iter 123 : loss : 0.3793 = 0.0989 + 0.1351 + 0.0101 + 0.1352, time: 20.533092]
2023-08-31 18:37:14.954: epoch 123:	0.04391592  	0.09588360  	0.08114129  
2023-08-31 18:37:35.534: [iter 124 : loss : 0.3774 = 0.0978 + 0.1346 + 0.0102 + 0.1347, time: 20.563768]
2023-08-31 18:37:36.209: epoch 124:	0.04383852  	0.09575605  	0.08106510  
2023-08-31 18:37:56.559: [iter 125 : loss : 0.3772 = 0.0987 + 0.1341 + 0.0102 + 0.1342, time: 20.338662]
2023-08-31 18:37:57.234: epoch 125:	0.04387724  	0.09583681  	0.08123711  
2023-08-31 18:38:17.537: [iter 126 : loss : 0.3764 = 0.0987 + 0.1336 + 0.0103 + 0.1338, time: 20.291906]
2023-08-31 18:38:18.223: epoch 126:	0.04383206  	0.09564389  	0.08106609  
2023-08-31 18:38:38.677: [iter 127 : loss : 0.3777 = 0.1009 + 0.1331 + 0.0104 + 0.1333, time: 20.441793]
2023-08-31 18:38:39.358: epoch 127:	0.04386432  	0.09573277  	0.08117013  
2023-08-31 18:38:59.523: [iter 128 : loss : 0.3757 = 0.0996 + 0.1327 + 0.0104 + 0.1329, time: 20.155624]
2023-08-31 18:39:00.197: epoch 128:	0.04387076  	0.09554189  	0.08111424  
2023-08-31 18:39:20.694: [iter 129 : loss : 0.3735 = 0.0982 + 0.1323 + 0.0105 + 0.1325, time: 20.477093]
2023-08-31 18:39:21.376: epoch 129:	0.04377402  	0.09535608  	0.08091845  
2023-08-31 18:39:21.376: Early stopping is trigger at epoch: 129
2023-08-31 18:39:21.376: best_result@epoch 119:

2023-08-31 18:39:21.376: 		0.0440      	0.0961      	0.0812      
