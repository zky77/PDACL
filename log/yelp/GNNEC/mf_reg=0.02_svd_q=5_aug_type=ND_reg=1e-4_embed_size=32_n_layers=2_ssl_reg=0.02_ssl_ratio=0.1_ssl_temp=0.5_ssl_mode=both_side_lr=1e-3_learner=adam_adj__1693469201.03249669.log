2023-08-31 16:06:41.032: my pid: 40683
2023-08-31 16:06:41.033: model: model.general_recommender.GNNEC
2023-08-31 16:06:41.033: Dataset statistics:
Name: yelp
The number of users: 7750
The number of items: 28918
The number of ratings: 750318
Average actions of users: 96.82
Average actions of items: 25.95
The sparsity of the dataset: 99.665208%

The number of training: 678579
The number of validation: 0
The number of testing: 71739
2023-08-31 16:06:41.033: NeuRec:[NeuRec]:
recommender=GNNEC
dataset=yelp
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

GNNEC:[hyperparameters]:
mf_reg=0.02
svd_q=5
aug_type=ND
reg=1e-4
embed_size=32
n_layers=2
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
ssl_mode=both_side
lr=1e-3
learner=adam
adj_type=pre
epochs=200
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=10
pretrain_flag=0
save_flag=0

Command line:
recommender=GNNEC
dataset=yelp
epochs=200
n_layers=2
embed_size=32
batch_size=2048
lr=1e-3
aug_type=ND
reg=1e-4
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
mf_reg=0.02
svd_q=5
2023-08-31 16:07:00.503: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-08-31 16:07:26.120: [iter 1 : loss : 1.2354 = 0.5425 + 0.3457 + 0.0001 + 0.3471, time: 25.616994]
2023-08-31 16:07:26.796: epoch 1:	0.02543924  	0.05491652  	0.04683334  
2023-08-31 16:07:26.797: Find a better model.
2023-08-31 16:07:51.936: [iter 2 : loss : 0.9846 = 0.3494 + 0.3164 + 0.0002 + 0.3186, time: 25.111462]
2023-08-31 16:07:52.588: epoch 2:	0.02541343  	0.05477874  	0.04652544  
2023-08-31 16:08:17.653: [iter 3 : loss : 0.9325 = 0.3113 + 0.3092 + 0.0003 + 0.3117, time: 25.039920]
2023-08-31 16:08:18.306: epoch 3:	0.02560052  	0.05542291  	0.04686618  
2023-08-31 16:08:18.306: Find a better model.
2023-08-31 16:08:43.307: [iter 4 : loss : 0.9021 = 0.2900 + 0.3043 + 0.0004 + 0.3074, time: 24.976503]
2023-08-31 16:08:43.954: epoch 4:	0.02643281  	0.05713124  	0.04821762  
2023-08-31 16:08:43.954: Find a better model.
2023-08-31 16:09:09.009: [iter 5 : loss : 0.8803 = 0.2755 + 0.3005 + 0.0005 + 0.3039, time: 25.029646]
2023-08-31 16:09:09.684: epoch 5:	0.02692960  	0.05833281  	0.04910279  
2023-08-31 16:09:09.685: Find a better model.
2023-08-31 16:09:34.923: [iter 6 : loss : 0.8614 = 0.2631 + 0.2971 + 0.0006 + 0.3007, time: 25.212506]
2023-08-31 16:09:35.594: epoch 6:	0.02727156  	0.05898453  	0.04972946  
2023-08-31 16:09:35.594: Find a better model.
2023-08-31 16:10:00.641: [iter 7 : loss : 0.8414 = 0.2492 + 0.2939 + 0.0007 + 0.2977, time: 25.025850]
2023-08-31 16:10:01.339: epoch 7:	0.02769739  	0.05972311  	0.05050164  
2023-08-31 16:10:01.339: Find a better model.
2023-08-31 16:10:26.470: [iter 8 : loss : 0.8256 = 0.2389 + 0.2910 + 0.0007 + 0.2949, time: 25.103075]
2023-08-31 16:10:27.068: epoch 8:	0.02811030  	0.06062869  	0.05117993  
2023-08-31 16:10:27.069: Find a better model.
2023-08-31 16:10:52.126: [iter 9 : loss : 0.8118 = 0.2304 + 0.2884 + 0.0008 + 0.2922, time: 25.036236]
2023-08-31 16:10:52.734: epoch 9:	0.02844582  	0.06130939  	0.05151068  
2023-08-31 16:10:52.734: Find a better model.
2023-08-31 16:11:17.786: [iter 10 : loss : 0.7987 = 0.2227 + 0.2856 + 0.0009 + 0.2895, time: 25.028384]
2023-08-31 16:11:18.427: epoch 10:	0.02854903  	0.06143351  	0.05171230  
2023-08-31 16:11:18.428: Find a better model.
2023-08-31 16:11:43.442: [iter 11 : loss : 0.7863 = 0.2155 + 0.2831 + 0.0010 + 0.2868, time: 24.993636]
2023-08-31 16:11:44.139: epoch 11:	0.02922001  	0.06302471  	0.05291833  
2023-08-31 16:11:44.140: Find a better model.
2023-08-31 16:12:09.380: [iter 12 : loss : 0.7734 = 0.2083 + 0.2802 + 0.0011 + 0.2839, time: 25.217833]
2023-08-31 16:12:10.029: epoch 12:	0.02975550  	0.06412588  	0.05375490  
2023-08-31 16:12:10.030: Find a better model.
2023-08-31 16:12:35.315: [iter 13 : loss : 0.7622 = 0.2027 + 0.2775 + 0.0011 + 0.2809, time: 25.264479]
2023-08-31 16:12:35.986: epoch 13:	0.03023940  	0.06518873  	0.05463625  
2023-08-31 16:12:35.986: Find a better model.
2023-08-31 16:13:01.031: [iter 14 : loss : 0.7485 = 0.1948 + 0.2745 + 0.0012 + 0.2779, time: 25.019874]
2023-08-31 16:13:01.687: epoch 14:	0.03089749  	0.06655022  	0.05563314  
2023-08-31 16:13:01.687: Find a better model.
2023-08-31 16:13:26.883: [iter 15 : loss : 0.7388 = 0.1911 + 0.2716 + 0.0013 + 0.2748, time: 25.179072]
2023-08-31 16:13:27.549: epoch 15:	0.03140722  	0.06808413  	0.05641064  
2023-08-31 16:13:27.549: Find a better model.
2023-08-31 16:13:52.713: [iter 16 : loss : 0.7281 = 0.1864 + 0.2686 + 0.0014 + 0.2716, time: 25.138033]
2023-08-31 16:13:53.370: epoch 16:	0.03186527  	0.06894250  	0.05705735  
2023-08-31 16:13:53.370: Find a better model.
2023-08-31 16:14:18.444: [iter 17 : loss : 0.7161 = 0.1806 + 0.2655 + 0.0015 + 0.2684, time: 25.054231]
2023-08-31 16:14:19.120: epoch 17:	0.03240724  	0.07020128  	0.05834781  
2023-08-31 16:14:19.120: Find a better model.
2023-08-31 16:14:44.296: [iter 18 : loss : 0.7064 = 0.1771 + 0.2624 + 0.0016 + 0.2652, time: 25.153193]
2023-08-31 16:14:44.929: epoch 18:	0.03294274  	0.07137764  	0.05917227  
2023-08-31 16:14:44.929: Find a better model.
2023-08-31 16:15:10.260: [iter 19 : loss : 0.6949 = 0.1721 + 0.2592 + 0.0017 + 0.2619, time: 25.309195]
2023-08-31 16:15:10.951: epoch 19:	0.03333624  	0.07222094  	0.05978147  
2023-08-31 16:15:10.951: Find a better model.
2023-08-31 16:15:35.977: [iter 20 : loss : 0.6849 = 0.1682 + 0.2562 + 0.0019 + 0.2587, time: 25.002366]
2023-08-31 16:15:36.617: epoch 20:	0.03387813  	0.07359568  	0.06100907  
2023-08-31 16:15:36.617: Find a better model.
2023-08-31 16:16:01.806: [iter 21 : loss : 0.6739 = 0.1636 + 0.2529 + 0.0020 + 0.2554, time: 25.170292]
2023-08-31 16:16:02.495: epoch 21:	0.03442647  	0.07476816  	0.06217954  
2023-08-31 16:16:02.496: Find a better model.
2023-08-31 16:16:27.657: [iter 22 : loss : 0.6644 = 0.1602 + 0.2500 + 0.0021 + 0.2522, time: 25.131943]
2023-08-31 16:16:28.301: epoch 22:	0.03494252  	0.07586260  	0.06314509  
2023-08-31 16:16:28.301: Find a better model.
2023-08-31 16:16:53.600: [iter 23 : loss : 0.6547 = 0.1567 + 0.2468 + 0.0022 + 0.2490, time: 25.276741]
2023-08-31 16:16:54.255: epoch 23:	0.03529736  	0.07667451  	0.06386350  
2023-08-31 16:16:54.256: Find a better model.
2023-08-31 16:17:19.388: [iter 24 : loss : 0.6464 = 0.1546 + 0.2437 + 0.0023 + 0.2458, time: 25.100429]
2023-08-31 16:17:20.074: epoch 24:	0.03568445  	0.07747951  	0.06467135  
2023-08-31 16:17:20.074: Find a better model.
2023-08-31 16:17:45.214: [iter 25 : loss : 0.6371 = 0.1512 + 0.2408 + 0.0024 + 0.2427, time: 25.118064]
2023-08-31 16:17:45.824: epoch 25:	0.03615532  	0.07841769  	0.06546346  
2023-08-31 16:17:45.824: Find a better model.
2023-08-31 16:18:10.897: [iter 26 : loss : 0.6288 = 0.1488 + 0.2378 + 0.0026 + 0.2396, time: 25.050561]
2023-08-31 16:18:11.555: epoch 26:	0.03661333  	0.07938494  	0.06626812  
2023-08-31 16:18:11.556: Find a better model.
2023-08-31 16:18:36.650: [iter 27 : loss : 0.6194 = 0.1454 + 0.2348 + 0.0027 + 0.2365, time: 25.067980]
2023-08-31 16:18:37.280: epoch 27:	0.03701975  	0.08041702  	0.06711963  
2023-08-31 16:18:37.280: Find a better model.
2023-08-31 16:19:02.353: [iter 28 : loss : 0.6118 = 0.1435 + 0.2319 + 0.0028 + 0.2336, time: 25.047728]
2023-08-31 16:19:03.009: epoch 28:	0.03744552  	0.08117081  	0.06786037  
2023-08-31 16:19:03.010: Find a better model.
2023-08-31 16:19:28.024: [iter 29 : loss : 0.6047 = 0.1421 + 0.2290 + 0.0030 + 0.2306, time: 24.983598]
2023-08-31 16:19:28.655: epoch 29:	0.03770358  	0.08177702  	0.06856103  
2023-08-31 16:19:28.655: Find a better model.
2023-08-31 16:19:53.678: [iter 30 : loss : 0.5970 = 0.1397 + 0.2264 + 0.0031 + 0.2278, time: 25.000757]
2023-08-31 16:19:54.281: epoch 30:	0.03812290  	0.08290292  	0.06935952  
2023-08-31 16:19:54.281: Find a better model.
2023-08-31 16:20:19.588: [iter 31 : loss : 0.5890 = 0.1373 + 0.2236 + 0.0032 + 0.2249, time: 25.285607]
2023-08-31 16:20:20.271: epoch 31:	0.03838094  	0.08328677  	0.06981295  
2023-08-31 16:20:20.271: Find a better model.
2023-08-31 16:20:45.271: [iter 32 : loss : 0.5806 = 0.1344 + 0.2208 + 0.0033 + 0.2221, time: 24.972834]
2023-08-31 16:20:45.936: epoch 32:	0.03872931  	0.08404753  	0.07054282  
2023-08-31 16:20:45.937: Find a better model.
2023-08-31 16:21:11.258: [iter 33 : loss : 0.5741 = 0.1331 + 0.2181 + 0.0035 + 0.2194, time: 25.296811]
2023-08-31 16:21:11.925: epoch 33:	0.03907121  	0.08487615  	0.07102375  
2023-08-31 16:21:11.925: Find a better model.
2023-08-31 16:21:37.221: [iter 34 : loss : 0.5687 = 0.1329 + 0.2154 + 0.0036 + 0.2167, time: 25.270676]
2023-08-31 16:21:37.871: epoch 34:	0.03950341  	0.08588360  	0.07175042  
2023-08-31 16:21:37.871: Find a better model.
2023-08-31 16:22:03.089: [iter 35 : loss : 0.5619 = 0.1311 + 0.2129 + 0.0037 + 0.2141, time: 25.179680]
2023-08-31 16:22:03.735: epoch 35:	0.03970983  	0.08619363  	0.07216653  
2023-08-31 16:22:03.735: Find a better model.
2023-08-31 16:22:28.819: [iter 36 : loss : 0.5557 = 0.1301 + 0.2103 + 0.0039 + 0.2115, time: 25.061666]
2023-08-31 16:22:29.455: epoch 36:	0.03996141  	0.08686768  	0.07278995  
2023-08-31 16:22:29.455: Find a better model.
2023-08-31 16:22:54.625: [iter 37 : loss : 0.5486 = 0.1278 + 0.2079 + 0.0040 + 0.2090, time: 25.146756]
2023-08-31 16:22:55.229: epoch 37:	0.04035494  	0.08794232  	0.07329236  
2023-08-31 16:22:55.229: Find a better model.
2023-08-31 16:23:20.186: [iter 38 : loss : 0.5426 = 0.1262 + 0.2057 + 0.0041 + 0.2065, time: 24.933476]
2023-08-31 16:23:20.842: epoch 38:	0.04049686  	0.08835316  	0.07370400  
2023-08-31 16:23:20.842: Find a better model.
2023-08-31 16:23:45.846: [iter 39 : loss : 0.5368 = 0.1254 + 0.2031 + 0.0043 + 0.2041, time: 24.973992]
2023-08-31 16:23:46.496: epoch 39:	0.04055491  	0.08860367  	0.07393060  
2023-08-31 16:23:46.497: Find a better model.
2023-08-31 16:24:11.573: [iter 40 : loss : 0.5307 = 0.1240 + 0.2007 + 0.0044 + 0.2017, time: 25.055224]
2023-08-31 16:24:12.257: epoch 40:	0.04076134  	0.08908997  	0.07432122  
2023-08-31 16:24:12.257: Find a better model.
2023-08-31 16:24:37.398: [iter 41 : loss : 0.5249 = 0.1226 + 0.1984 + 0.0046 + 0.1994, time: 25.115416]
2023-08-31 16:24:38.077: epoch 41:	0.04087747  	0.08929681  	0.07464764  
2023-08-31 16:24:38.078: Find a better model.
2023-08-31 16:25:03.340: [iter 42 : loss : 0.5192 = 0.1213 + 0.1962 + 0.0047 + 0.1970, time: 25.237323]
2023-08-31 16:25:03.983: epoch 42:	0.04101933  	0.08944240  	0.07484089  
2023-08-31 16:25:03.983: Find a better model.
2023-08-31 16:25:29.102: [iter 43 : loss : 0.5133 = 0.1199 + 0.1939 + 0.0048 + 0.1947, time: 25.100317]
2023-08-31 16:25:29.744: epoch 43:	0.04097418  	0.08954052  	0.07501398  
2023-08-31 16:25:29.744: Find a better model.
2023-08-31 16:25:54.954: [iter 44 : loss : 0.5087 = 0.1196 + 0.1916 + 0.0050 + 0.1925, time: 25.184461]
2023-08-31 16:25:55.597: epoch 44:	0.04118707  	0.08991766  	0.07545602  
2023-08-31 16:25:55.598: Find a better model.
2023-08-31 16:26:20.762: [iter 45 : loss : 0.5036 = 0.1187 + 0.1895 + 0.0051 + 0.1903, time: 25.139508]
2023-08-31 16:26:21.445: epoch 45:	0.04146447  	0.09051891  	0.07575216  
2023-08-31 16:26:21.445: Find a better model.
2023-08-31 16:26:46.413: [iter 46 : loss : 0.4995 = 0.1187 + 0.1874 + 0.0052 + 0.1882, time: 24.946332]
2023-08-31 16:26:47.039: epoch 46:	0.04167091  	0.09087788  	0.07603081  
2023-08-31 16:26:47.039: Find a better model.
2023-08-31 16:27:12.059: [iter 47 : loss : 0.4934 = 0.1167 + 0.1852 + 0.0054 + 0.1861, time: 24.997479]
2023-08-31 16:27:12.714: epoch 47:	0.04183217  	0.09123360  	0.07614609  
2023-08-31 16:27:12.714: Find a better model.
2023-08-31 16:27:37.671: [iter 48 : loss : 0.4896 = 0.1167 + 0.1833 + 0.0055 + 0.1840, time: 24.932626]
2023-08-31 16:27:38.265: epoch 48:	0.04203860  	0.09179472  	0.07673073  
2023-08-31 16:27:38.265: Find a better model.
2023-08-31 16:28:03.294: [iter 49 : loss : 0.4847 = 0.1158 + 0.1813 + 0.0057 + 0.1820, time: 25.006714]
2023-08-31 16:28:03.926: epoch 49:	0.04216115  	0.09206447  	0.07721945  
2023-08-31 16:28:03.926: Find a better model.
2023-08-31 16:28:29.017: [iter 50 : loss : 0.4807 = 0.1154 + 0.1794 + 0.0058 + 0.1801, time: 25.069652]
2023-08-31 16:28:29.664: epoch 50:	0.04220629  	0.09213421  	0.07716636  
2023-08-31 16:28:29.665: Find a better model.
2023-08-31 16:28:54.826: [iter 51 : loss : 0.4762 = 0.1146 + 0.1775 + 0.0059 + 0.1782, time: 25.137839]
2023-08-31 16:28:55.477: epoch 51:	0.04223856  	0.09222451  	0.07739525  
2023-08-31 16:28:55.477: Find a better model.
2023-08-31 16:29:20.532: [iter 52 : loss : 0.4719 = 0.1138 + 0.1757 + 0.0061 + 0.1763, time: 25.029163]
2023-08-31 16:29:21.215: epoch 52:	0.04235468  	0.09242003  	0.07762190  
2023-08-31 16:29:21.215: Find a better model.
2023-08-31 16:29:46.228: [iter 53 : loss : 0.4684 = 0.1139 + 0.1738 + 0.0062 + 0.1745, time: 24.996960]
2023-08-31 16:29:46.851: epoch 53:	0.04257404  	0.09286516  	0.07787389  
2023-08-31 16:29:46.852: Find a better model.
2023-08-31 16:30:11.914: [iter 54 : loss : 0.4637 = 0.1124 + 0.1722 + 0.0063 + 0.1728, time: 25.041292]
2023-08-31 16:30:12.561: epoch 54:	0.04274822  	0.09337382  	0.07819414  
2023-08-31 16:30:12.561: Find a better model.
2023-08-31 16:30:37.651: [iter 55 : loss : 0.4599 = 0.1121 + 0.1704 + 0.0065 + 0.1710, time: 25.069983]
2023-08-31 16:30:38.329: epoch 55:	0.04252244  	0.09274410  	0.07789291  
2023-08-31 16:31:03.289: [iter 56 : loss : 0.4563 = 0.1118 + 0.1687 + 0.0066 + 0.1692, time: 24.929674]
2023-08-31 16:31:03.945: epoch 56:	0.04273531  	0.09327127  	0.07829145  
2023-08-31 16:31:29.217: [iter 57 : loss : 0.4531 = 0.1117 + 0.1671 + 0.0067 + 0.1676, time: 25.253422]
2023-08-31 16:31:29.866: epoch 57:	0.04281918  	0.09355202  	0.07834571  
2023-08-31 16:31:29.866: Find a better model.
2023-08-31 16:31:54.865: [iter 58 : loss : 0.4507 = 0.1121 + 0.1656 + 0.0069 + 0.1661, time: 24.973740]
2023-08-31 16:31:55.512: epoch 58:	0.04285787  	0.09378987  	0.07850883  
2023-08-31 16:31:55.512: Find a better model.
2023-08-31 16:32:20.671: [iter 59 : loss : 0.4468 = 0.1114 + 0.1639 + 0.0070 + 0.1645, time: 25.138407]
2023-08-31 16:32:21.331: epoch 59:	0.04302562  	0.09394598  	0.07887349  
2023-08-31 16:32:21.331: Find a better model.
2023-08-31 16:32:46.440: [iter 60 : loss : 0.4428 = 0.1100 + 0.1626 + 0.0071 + 0.1630, time: 25.085978]
2023-08-31 16:32:47.125: epoch 60:	0.04325143  	0.09445286  	0.07912476  
2023-08-31 16:32:47.126: Find a better model.
2023-08-31 16:33:12.282: [iter 61 : loss : 0.4393 = 0.1096 + 0.1610 + 0.0072 + 0.1615, time: 25.130599]
2023-08-31 16:33:12.941: epoch 61:	0.04331595  	0.09451373  	0.07928371  
2023-08-31 16:33:12.941: Find a better model.
2023-08-31 16:33:38.122: [iter 62 : loss : 0.4370 = 0.1100 + 0.1595 + 0.0074 + 0.1600, time: 25.159708]
2023-08-31 16:33:38.769: epoch 62:	0.04327079  	0.09431366  	0.07929073  
2023-08-31 16:34:03.933: [iter 63 : loss : 0.4327 = 0.1083 + 0.1582 + 0.0075 + 0.1587, time: 25.138244]
2023-08-31 16:34:04.554: epoch 63:	0.04333530  	0.09453940  	0.07945312  
2023-08-31 16:34:04.554: Find a better model.
2023-08-31 16:34:29.560: [iter 64 : loss : 0.4297 = 0.1080 + 0.1569 + 0.0076 + 0.1573, time: 24.988777]
2023-08-31 16:34:30.204: epoch 64:	0.04343854  	0.09486826  	0.07967611  
2023-08-31 16:34:30.204: Find a better model.
2023-08-31 16:34:55.212: [iter 65 : loss : 0.4281 = 0.1089 + 0.1555 + 0.0077 + 0.1559, time: 24.983145]
2023-08-31 16:34:55.867: epoch 65:	0.04335468  	0.09467528  	0.07959301  
2023-08-31 16:35:20.891: [iter 66 : loss : 0.4234 = 0.1066 + 0.1543 + 0.0079 + 0.1547, time: 24.988356]
2023-08-31 16:35:21.534: epoch 66:	0.04364498  	0.09535800  	0.07996461  
2023-08-31 16:35:21.534: Find a better model.
2023-08-31 16:35:46.577: [iter 67 : loss : 0.4209 = 0.1065 + 0.1530 + 0.0080 + 0.1534, time: 25.015315]
2023-08-31 16:35:47.225: epoch 67:	0.04361273  	0.09521792  	0.08003469  
2023-08-31 16:36:12.323: [iter 68 : loss : 0.4200 = 0.1079 + 0.1518 + 0.0081 + 0.1521, time: 25.072071]
2023-08-31 16:36:12.979: epoch 68:	0.04364498  	0.09526014  	0.08019432  
2023-08-31 16:36:37.812: [iter 69 : loss : 0.4182 = 0.1083 + 0.1507 + 0.0082 + 0.1510, time: 24.805324]
2023-08-31 16:36:38.473: epoch 69:	0.04371593  	0.09534694  	0.08024962  
2023-08-31 16:37:03.513: [iter 70 : loss : 0.4150 = 0.1072 + 0.1496 + 0.0083 + 0.1498, time: 25.016790]
2023-08-31 16:37:04.157: epoch 70:	0.04379981  	0.09561111  	0.08031782  
2023-08-31 16:37:04.157: Find a better model.
2023-08-31 16:37:29.082: [iter 71 : loss : 0.4114 = 0.1058 + 0.1484 + 0.0084 + 0.1487, time: 24.900873]
2023-08-31 16:37:29.722: epoch 71:	0.04361917  	0.09518650  	0.08024249  
2023-08-31 16:37:54.783: [iter 72 : loss : 0.4100 = 0.1065 + 0.1472 + 0.0086 + 0.1477, time: 25.040610]
2023-08-31 16:37:55.428: epoch 72:	0.04362560  	0.09522792  	0.08015513  
2023-08-31 16:38:20.327: [iter 73 : loss : 0.4068 = 0.1053 + 0.1463 + 0.0087 + 0.1466, time: 24.877086]
2023-08-31 16:38:20.931: epoch 73:	0.04359338  	0.09509575  	0.08007964  
2023-08-31 16:38:46.077: [iter 74 : loss : 0.4043 = 0.1049 + 0.1451 + 0.0088 + 0.1455, time: 25.125294]
2023-08-31 16:38:46.721: epoch 74:	0.04367724  	0.09545930  	0.08025993  
2023-08-31 16:39:11.631: [iter 75 : loss : 0.4028 = 0.1053 + 0.1441 + 0.0089 + 0.1445, time: 24.887621]
2023-08-31 16:39:12.304: epoch 75:	0.04355464  	0.09512900  	0.08008943  
2023-08-31 16:39:37.110: [iter 76 : loss : 0.4015 = 0.1056 + 0.1434 + 0.0090 + 0.1436, time: 24.782984]
2023-08-31 16:39:37.718: epoch 76:	0.04366430  	0.09537333  	0.08034411  
2023-08-31 16:40:02.490: [iter 77 : loss : 0.3985 = 0.1043 + 0.1425 + 0.0091 + 0.1427, time: 24.751981]
2023-08-31 16:40:03.138: epoch 77:	0.04365786  	0.09517206  	0.08024485  
2023-08-31 16:40:27.777: [iter 78 : loss : 0.3977 = 0.1052 + 0.1415 + 0.0092 + 0.1417, time: 24.618324]
2023-08-31 16:40:28.426: epoch 78:	0.04374174  	0.09539397  	0.08023369  
2023-08-31 16:40:53.172: [iter 79 : loss : 0.3957 = 0.1048 + 0.1407 + 0.0093 + 0.1409, time: 24.728645]
2023-08-31 16:40:53.786: epoch 79:	0.04359335  	0.09496062  	0.08018491  
2023-08-31 16:41:18.480: [iter 80 : loss : 0.3936 = 0.1043 + 0.1398 + 0.0094 + 0.1400, time: 24.666973]
2023-08-31 16:41:19.124: epoch 80:	0.04355463  	0.09496965  	0.08006854  
2023-08-31 16:41:19.124: Early stopping is trigger at epoch: 80
2023-08-31 16:41:19.125: best_result@epoch 70:

2023-08-31 16:41:19.125: 		0.0438      	0.0956      	0.0803      
