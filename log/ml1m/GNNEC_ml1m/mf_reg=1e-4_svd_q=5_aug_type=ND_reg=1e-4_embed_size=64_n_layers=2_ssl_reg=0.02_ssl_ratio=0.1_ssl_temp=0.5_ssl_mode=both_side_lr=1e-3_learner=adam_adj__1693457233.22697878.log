2023-08-31 12:47:13.227: my pid: 36230
2023-08-31 12:47:13.227: model: model.general_recommender.GNNEC_ml1m
2023-08-31 12:47:13.227: Dataset statistics:
Name: ml1m
The number of users: 6040
The number of items: 3706
The number of ratings: 1000209
Average actions of users: 165.60
Average actions of items: 269.89
The sparsity of the dataset: 95.531637%

The number of training: 902826
The number of validation: 0
The number of testing: 97383
2023-08-31 12:47:13.227: NeuRec:[NeuRec]:
recommender=GNNEC_ml1m
dataset=ml1m
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

GNNEC_ml1m:[hyperparameters]:
mf_reg=1e-4
svd_q=5
aug_type=ND
reg=1e-4
embed_size=64
n_layers=2
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
ssl_mode=both_side
lr=1e-3
learner=adam
adj_type=pre
epochs=100
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=10
pretrain_flag=0
save_flag=0

Command line:
recommender=GNNEC_ml1m
dataset=ml1m
epochs=100
n_layers=2
embed_size=64
batch_size=2048
lr=1e-3
aug_type=ND
reg=1e-4
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
mf_reg=1e-4
svd_q=5
2023-08-31 12:47:17.218: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-08-31 12:47:58.420: [iter 1 : loss : 0.9417 = 0.6831 + 0.2586 + 0.0000, time: 41.200806]
2023-08-31 12:47:58.683: epoch 1:	0.09412146  	0.15075016  	0.16199093  
2023-08-31 12:47:58.683: Find a better model.
2023-08-31 12:48:38.384: [iter 2 : loss : 0.6527 = 0.3526 + 0.2995 + 0.0006, time: 39.674586]
2023-08-31 12:48:38.652: epoch 2:	0.09904655  	0.15413888  	0.16569519  
2023-08-31 12:48:38.652: Find a better model.
2023-08-31 12:49:17.806: [iter 3 : loss : 0.5535 = 0.2582 + 0.2942 + 0.0010, time: 39.121891]
2023-08-31 12:49:18.074: epoch 3:	0.11516373  	0.18219653  	0.19354820  
2023-08-31 12:49:18.075: Find a better model.
2023-08-31 12:49:57.937: [iter 4 : loss : 0.5186 = 0.2285 + 0.2888 + 0.0013, time: 39.835828]
2023-08-31 12:49:58.147: epoch 4:	0.12437719  	0.19828883  	0.20909956  
2023-08-31 12:49:58.147: Find a better model.
2023-08-31 12:50:39.046: [iter 5 : loss : 0.4980 = 0.2107 + 0.2857 + 0.0015, time: 40.873089]
2023-08-31 12:50:39.305: epoch 5:	0.13080938  	0.20980591  	0.21980546  
2023-08-31 12:50:39.305: Find a better model.
2023-08-31 12:51:20.050: [iter 6 : loss : 0.4835 = 0.1981 + 0.2837 + 0.0018, time: 40.718652]
2023-08-31 12:51:20.319: epoch 6:	0.13503131  	0.21639270  	0.22672978  
2023-08-31 12:51:20.320: Find a better model.
2023-08-31 12:52:00.247: [iter 7 : loss : 0.4742 = 0.1903 + 0.2820 + 0.0020, time: 39.896456]
2023-08-31 12:52:00.517: epoch 7:	0.13892196  	0.22458716  	0.23336244  
2023-08-31 12:52:00.518: Find a better model.
2023-08-31 12:52:40.205: [iter 8 : loss : 0.4651 = 0.1823 + 0.2806 + 0.0021, time: 39.658005]
2023-08-31 12:52:40.474: epoch 8:	0.14147195  	0.22905664  	0.23819686  
2023-08-31 12:52:40.475: Find a better model.
2023-08-31 12:53:20.385: [iter 9 : loss : 0.4571 = 0.1754 + 0.2794 + 0.0023, time: 39.886132]
2023-08-31 12:53:20.630: epoch 9:	0.14421199  	0.23398526  	0.24327600  
2023-08-31 12:53:20.631: Find a better model.
2023-08-31 12:54:01.379: [iter 10 : loss : 0.4506 = 0.1698 + 0.2784 + 0.0025, time: 40.729026]
2023-08-31 12:54:01.666: epoch 10:	0.14625658  	0.23857082  	0.24713317  
2023-08-31 12:54:01.666: Find a better model.
2023-08-31 12:54:58.245: [iter 11 : loss : 0.4447 = 0.1646 + 0.2775 + 0.0026, time: 56.535244]
2023-08-31 12:54:58.614: epoch 11:	0.14821830  	0.24165219  	0.25075930  
2023-08-31 12:54:58.614: Find a better model.
2023-08-31 12:55:59.673: [iter 12 : loss : 0.4397 = 0.1603 + 0.2767 + 0.0028, time: 61.034981]
2023-08-31 12:56:00.038: epoch 12:	0.14984916  	0.24394679  	0.25303590  
2023-08-31 12:56:00.038: Find a better model.
2023-08-31 12:57:00.852: [iter 13 : loss : 0.4351 = 0.1562 + 0.2759 + 0.0029, time: 60.790481]
2023-08-31 12:57:01.212: epoch 13:	0.15116544  	0.24709313  	0.25583595  
2023-08-31 12:57:01.212: Find a better model.
2023-08-31 12:58:02.440: [iter 14 : loss : 0.4300 = 0.1516 + 0.2752 + 0.0031, time: 61.205304]
2023-08-31 12:58:02.771: epoch 14:	0.15262234  	0.24960749  	0.25857264  
2023-08-31 12:58:02.772: Find a better model.
2023-08-31 12:59:03.837: [iter 15 : loss : 0.4268 = 0.1490 + 0.2746 + 0.0032, time: 61.042574]
2023-08-31 12:59:04.210: epoch 15:	0.15356600  	0.25099105  	0.26034784  
2023-08-31 12:59:04.210: Find a better model.
2023-08-31 13:00:04.066: [iter 16 : loss : 0.4224 = 0.1450 + 0.2740 + 0.0033, time: 59.837959]
2023-08-31 13:00:04.318: epoch 16:	0.15453447  	0.25298202  	0.26211837  
2023-08-31 13:00:04.318: Find a better model.
2023-08-31 13:01:05.758: [iter 17 : loss : 0.4194 = 0.1425 + 0.2735 + 0.0035, time: 61.417870]
2023-08-31 13:01:06.097: epoch 17:	0.15537070  	0.25407770  	0.26348418  
2023-08-31 13:01:06.097: Find a better model.
2023-08-31 13:02:07.410: [iter 18 : loss : 0.4161 = 0.1395 + 0.2730 + 0.0036, time: 61.293776]
2023-08-31 13:02:07.764: epoch 18:	0.15619847  	0.25542936  	0.26541251  
2023-08-31 13:02:07.765: Find a better model.
2023-08-31 13:03:07.758: [iter 19 : loss : 0.4134 = 0.1372 + 0.2725 + 0.0037, time: 59.971839]
2023-08-31 13:03:08.035: epoch 19:	0.15718368  	0.25724036  	0.26703048  
2023-08-31 13:03:08.035: Find a better model.
2023-08-31 13:04:09.391: [iter 20 : loss : 0.4103 = 0.1344 + 0.2721 + 0.0039, time: 61.327263]
2023-08-31 13:04:09.728: epoch 20:	0.15804462  	0.25899476  	0.26820511  
2023-08-31 13:04:09.729: Find a better model.
2023-08-31 13:05:10.482: [iter 21 : loss : 0.4078 = 0.1322 + 0.2717 + 0.0040, time: 60.730930]
2023-08-31 13:05:10.841: epoch 21:	0.15891388  	0.26068413  	0.26963520  
2023-08-31 13:05:10.841: Find a better model.
2023-08-31 13:06:23.048: [iter 22 : loss : 0.4052 = 0.1298 + 0.2713 + 0.0041, time: 72.186726]
2023-08-31 13:06:23.372: epoch 22:	0.15996526  	0.26280227  	0.27091813  
2023-08-31 13:06:23.372: Find a better model.
2023-08-31 13:07:24.887: [iter 23 : loss : 0.4019 = 0.1267 + 0.2710 + 0.0043, time: 61.493823]
2023-08-31 13:07:25.264: epoch 23:	0.16032946  	0.26380292  	0.27202937  
2023-08-31 13:07:25.264: Find a better model.
2023-08-31 13:08:12.434: [iter 24 : loss : 0.4006 = 0.1255 + 0.2706 + 0.0044, time: 47.143469]
2023-08-31 13:08:12.696: epoch 24:	0.16122334  	0.26640284  	0.27372569  
2023-08-31 13:08:12.697: Find a better model.
2023-08-31 13:08:58.477: [iter 25 : loss : 0.3994 = 0.1246 + 0.2703 + 0.0045, time: 45.760823]
2023-08-31 13:08:58.790: epoch 25:	0.16153803  	0.26705921  	0.27406123  
2023-08-31 13:08:58.791: Find a better model.
2023-08-31 13:09:44.381: [iter 26 : loss : 0.3961 = 0.1215 + 0.2699 + 0.0046, time: 45.565591]
2023-08-31 13:09:44.677: epoch 26:	0.16200152  	0.26787078  	0.27554625  
2023-08-31 13:09:44.678: Find a better model.
2023-08-31 13:10:30.312: [iter 27 : loss : 0.3943 = 0.1198 + 0.2697 + 0.0047, time: 45.605698]
2023-08-31 13:10:30.616: epoch 27:	0.16248158  	0.26891154  	0.27631620  
2023-08-31 13:10:30.617: Find a better model.
2023-08-31 13:11:16.069: [iter 28 : loss : 0.3924 = 0.1181 + 0.2694 + 0.0049, time: 45.426874]
2023-08-31 13:11:16.371: epoch 28:	0.16293682  	0.26957291  	0.27750856  
2023-08-31 13:11:16.371: Find a better model.
2023-08-31 13:12:01.755: [iter 29 : loss : 0.3908 = 0.1167 + 0.2692 + 0.0050, time: 45.358084]
2023-08-31 13:12:02.060: epoch 29:	0.16316032  	0.26971242  	0.27736771  
2023-08-31 13:12:02.061: Find a better model.
2023-08-31 13:12:47.700: [iter 30 : loss : 0.3883 = 0.1142 + 0.2689 + 0.0051, time: 45.618362]
2023-08-31 13:12:48.012: epoch 30:	0.16366540  	0.27119631  	0.27864075  
2023-08-31 13:12:48.012: Find a better model.
2023-08-31 13:13:33.750: [iter 31 : loss : 0.3871 = 0.1131 + 0.2687 + 0.0052, time: 45.720004]
2023-08-31 13:13:34.063: epoch 31:	0.16399647  	0.27154222  	0.27848491  
2023-08-31 13:13:34.063: Find a better model.
2023-08-31 13:14:19.960: [iter 32 : loss : 0.3857 = 0.1118 + 0.2686 + 0.0053, time: 45.879432]
2023-08-31 13:14:20.265: epoch 32:	0.16407102  	0.27199471  	0.27878803  
2023-08-31 13:14:20.265: Find a better model.
2023-08-31 13:15:05.887: [iter 33 : loss : 0.3844 = 0.1108 + 0.2682 + 0.0054, time: 45.589228]
2023-08-31 13:15:06.194: epoch 33:	0.16489042  	0.27273238  	0.27940717  
2023-08-31 13:15:06.195: Find a better model.
2023-08-31 13:15:51.863: [iter 34 : loss : 0.3827 = 0.1092 + 0.2681 + 0.0055, time: 45.649598]
2023-08-31 13:15:52.166: epoch 34:	0.16523004  	0.27361292  	0.27985874  
2023-08-31 13:15:52.166: Find a better model.
2023-08-31 13:16:37.975: [iter 35 : loss : 0.3811 = 0.1076 + 0.2679 + 0.0056, time: 45.784103]
2023-08-31 13:16:38.269: epoch 35:	0.16503955  	0.27356225  	0.28027800  
2023-08-31 13:17:24.026: [iter 36 : loss : 0.3805 = 0.1071 + 0.2677 + 0.0057, time: 45.730962]
2023-08-31 13:17:24.333: epoch 36:	0.16530454  	0.27357605  	0.28056666  
2023-08-31 13:18:10.031: [iter 37 : loss : 0.3796 = 0.1063 + 0.2675 + 0.0058, time: 45.676511]
2023-08-31 13:18:10.337: epoch 37:	0.16559418  	0.27444431  	0.28103936  
2023-08-31 13:18:10.338: Find a better model.
2023-08-31 13:18:56.373: [iter 38 : loss : 0.3786 = 0.1054 + 0.2674 + 0.0059, time: 46.017713]
2023-08-31 13:18:56.678: epoch 38:	0.16614883  	0.27483088  	0.28158084  
2023-08-31 13:18:56.678: Find a better model.
2023-08-31 13:19:42.027: [iter 39 : loss : 0.3767 = 0.1035 + 0.2672 + 0.0060, time: 45.323117]
2023-08-31 13:19:42.328: epoch 39:	0.16618201  	0.27500346  	0.28162730  
2023-08-31 13:19:42.329: Find a better model.
2023-08-31 13:20:27.761: [iter 40 : loss : 0.3752 = 0.1022 + 0.2670 + 0.0061, time: 45.408676]
2023-08-31 13:20:28.057: epoch 40:	0.16631465  	0.27507311  	0.28162527  
2023-08-31 13:20:28.057: Find a better model.
2023-08-31 13:21:13.635: [iter 41 : loss : 0.3746 = 0.1015 + 0.2669 + 0.0062, time: 45.556308]
2023-08-31 13:21:13.933: epoch 41:	0.16666219  	0.27641106  	0.28223062  
2023-08-31 13:21:13.933: Find a better model.
2023-08-31 13:21:59.837: [iter 42 : loss : 0.3732 = 0.1003 + 0.2667 + 0.0063, time: 45.877309]
2023-08-31 13:22:00.130: epoch 42:	0.16681950  	0.27654222  	0.28234968  
2023-08-31 13:22:00.130: Find a better model.
2023-08-31 13:22:45.660: [iter 43 : loss : 0.3720 = 0.0991 + 0.2665 + 0.0064, time: 45.511233]
2023-08-31 13:22:45.970: epoch 43:	0.16724168  	0.27728373  	0.28272510  
2023-08-31 13:22:45.970: Find a better model.
2023-08-31 13:23:31.432: [iter 44 : loss : 0.3709 = 0.0979 + 0.2664 + 0.0065, time: 45.436762]
2023-08-31 13:23:31.731: epoch 44:	0.16730785  	0.27724990  	0.28325689  
2023-08-31 13:24:17.462: [iter 45 : loss : 0.3707 = 0.0978 + 0.2664 + 0.0066, time: 45.711181]
2023-08-31 13:24:17.768: epoch 45:	0.16738240  	0.27771673  	0.28332546  
2023-08-31 13:24:17.768: Find a better model.
2023-08-31 13:25:13.675: [iter 46 : loss : 0.3692 = 0.0963 + 0.2661 + 0.0067, time: 55.880423]
2023-08-31 13:25:13.971: epoch 46:	0.16763905  	0.27828923  	0.28394473  
2023-08-31 13:25:13.971: Find a better model.
2023-08-31 13:25:59.878: [iter 47 : loss : 0.3683 = 0.0955 + 0.2661 + 0.0068, time: 45.887517]
2023-08-31 13:26:00.173: epoch 47:	0.16787912  	0.27844715  	0.28420478  
2023-08-31 13:26:00.173: Find a better model.
2023-08-31 13:26:45.919: [iter 48 : loss : 0.3671 = 0.0942 + 0.2660 + 0.0069, time: 45.720021]
2023-08-31 13:26:46.225: epoch 48:	0.16794525  	0.27872780  	0.28459084  
2023-08-31 13:26:46.225: Find a better model.
2023-08-31 13:27:32.061: [iter 49 : loss : 0.3673 = 0.0946 + 0.2658 + 0.0070, time: 45.816490]
2023-08-31 13:27:32.371: epoch 49:	0.16826814  	0.27912793  	0.28499034  
2023-08-31 13:27:32.372: Find a better model.
2023-08-31 13:28:18.637: [iter 50 : loss : 0.3656 = 0.0928 + 0.2657 + 0.0070, time: 46.242697]
2023-08-31 13:28:18.930: epoch 50:	0.16864890  	0.27964967  	0.28547284  
2023-08-31 13:28:18.930: Find a better model.
2023-08-31 13:29:04.338: [iter 51 : loss : 0.3647 = 0.0920 + 0.2656 + 0.0071, time: 45.386425]
2023-08-31 13:29:04.648: epoch 51:	0.16894680  	0.27967778  	0.28558055  
2023-08-31 13:29:04.648: Find a better model.
2023-08-31 13:29:50.605: [iter 52 : loss : 0.3643 = 0.0917 + 0.2654 + 0.0072, time: 45.939219]
2023-08-31 13:29:50.918: epoch 52:	0.16902965  	0.27975884  	0.28579977  
2023-08-31 13:29:50.918: Find a better model.
2023-08-31 13:30:36.217: [iter 53 : loss : 0.3634 = 0.0908 + 0.2653 + 0.0073, time: 45.272939]
2023-08-31 13:30:36.516: epoch 53:	0.16912900  	0.27957982  	0.28527457  
2023-08-31 13:31:22.165: [iter 54 : loss : 0.3624 = 0.0898 + 0.2653 + 0.0074, time: 45.623077]
2023-08-31 13:31:22.463: epoch 54:	0.16917032  	0.27945614  	0.28571194  
2023-08-31 13:32:08.110: [iter 55 : loss : 0.3622 = 0.0895 + 0.2652 + 0.0075, time: 45.626566]
2023-08-31 13:32:08.375: epoch 55:	0.16926964  	0.28084984  	0.28579345  
2023-08-31 13:32:08.375: Find a better model.
2023-08-31 13:32:54.051: [iter 56 : loss : 0.3604 = 0.0878 + 0.2650 + 0.0076, time: 45.652562]
2023-08-31 13:32:54.350: epoch 56:	0.16917849  	0.27996382  	0.28535107  
2023-08-31 13:33:38.942: [iter 57 : loss : 0.3601 = 0.0875 + 0.2650 + 0.0076, time: 44.571198]
2023-08-31 13:33:39.224: epoch 57:	0.16928622  	0.27984387  	0.28535572  
2023-08-31 13:34:24.994: [iter 58 : loss : 0.3594 = 0.0868 + 0.2649 + 0.0077, time: 45.753070]
2023-08-31 13:34:25.273: epoch 58:	0.16940215  	0.27941722  	0.28540963  
2023-08-31 13:35:10.693: [iter 59 : loss : 0.3590 = 0.0864 + 0.2648 + 0.0078, time: 45.402889]
2023-08-31 13:35:10.959: epoch 59:	0.16944364  	0.28009456  	0.28540263  
2023-08-31 13:35:56.064: [iter 60 : loss : 0.3582 = 0.0857 + 0.2646 + 0.0079, time: 45.084210]
2023-08-31 13:35:56.347: epoch 60:	0.16945171  	0.28035322  	0.28561109  
2023-08-31 13:36:41.750: [iter 61 : loss : 0.3579 = 0.0853 + 0.2646 + 0.0080, time: 45.384561]
2023-08-31 13:36:42.028: epoch 61:	0.16948491  	0.27984330  	0.28545514  
2023-08-31 13:37:26.998: [iter 62 : loss : 0.3572 = 0.0847 + 0.2645 + 0.0080, time: 44.950231]
2023-08-31 13:37:27.279: epoch 62:	0.16971664  	0.27995905  	0.28581637  
2023-08-31 13:38:12.756: [iter 63 : loss : 0.3569 = 0.0843 + 0.2645 + 0.0081, time: 45.451099]
2023-08-31 13:38:13.077: epoch 63:	0.16985759  	0.27998483  	0.28574300  
2023-08-31 13:38:59.398: [iter 64 : loss : 0.3561 = 0.0836 + 0.2643 + 0.0082, time: 46.297587]
2023-08-31 13:38:59.689: epoch 64:	0.16997325  	0.28006873  	0.28547928  
2023-08-31 13:39:46.021: [iter 65 : loss : 0.3556 = 0.0831 + 0.2643 + 0.0083, time: 46.314393]
2023-08-31 13:39:46.324: epoch 65:	0.17041193  	0.28073257  	0.28577846  
2023-08-31 13:39:46.324: Early stopping is trigger at epoch: 65
2023-08-31 13:39:46.324: best_result@epoch 55:

2023-08-31 13:39:46.324: 		0.1693      	0.2808      	0.2858      
