2023-08-31 12:54:07.834: my pid: 605
2023-08-31 12:54:07.834: model: model.general_recommender.GNNEC_ml1m
2023-08-31 12:54:07.834: Dataset statistics:
Name: ml1m
The number of users: 6040
The number of items: 3706
The number of ratings: 1000209
Average actions of users: 165.60
Average actions of items: 269.89
The sparsity of the dataset: 95.531637%

The number of training: 902826
The number of validation: 0
The number of testing: 97383
2023-08-31 12:54:07.835: NeuRec:[NeuRec]:
recommender=GNNEC_ml1m
dataset=ml1m
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

GNNEC_ml1m:[hyperparameters]:
mf_reg=1e-4
svd_q=5
aug_type=ND
reg=1e-4
embed_size=64
n_layers=2
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
ssl_mode=both_side
lr=1e-3
learner=adam
adj_type=pre
epochs=100
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=10
pretrain_flag=0
save_flag=0

Command line:
recommender=GNNEC_ml1m
dataset=ml1m
epochs=100
n_layers=2
embed_size=64
batch_size=2048
lr=1e-3
aug_type=ND
reg=1e-4
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
mf_reg=1e-4
svd_q=5
2023-08-31 12:54:12.697: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-08-31 12:55:15.243: [iter 1 : loss : 0.9417 = 0.6831 + 0.2586 + 0.0000, time: 62.543513]
2023-08-31 12:55:15.597: epoch 1:	0.09412146  	0.15075016  	0.16199093  
2023-08-31 12:55:15.597: Find a better model.
2023-08-31 12:56:17.300: [iter 2 : loss : 0.6527 = 0.3526 + 0.2995 + 0.0006, time: 61.677844]
2023-08-31 12:56:17.655: epoch 2:	0.09904655  	0.15413888  	0.16569519  
2023-08-31 12:56:17.655: Find a better model.
2023-08-31 12:57:18.929: [iter 3 : loss : 0.5535 = 0.2582 + 0.2942 + 0.0010, time: 61.250321]
2023-08-31 12:57:19.269: epoch 3:	0.11516373  	0.18219653  	0.19354820  
2023-08-31 12:57:19.269: Find a better model.
2023-08-31 12:58:20.787: [iter 4 : loss : 0.5186 = 0.2285 + 0.2888 + 0.0013, time: 61.494407]
2023-08-31 12:58:21.142: epoch 4:	0.12437719  	0.19828883  	0.20909956  
2023-08-31 12:58:21.142: Find a better model.
2023-08-31 12:59:22.846: [iter 5 : loss : 0.4980 = 0.2107 + 0.2857 + 0.0015, time: 61.682042]
2023-08-31 12:59:23.195: epoch 5:	0.13080938  	0.20980591  	0.21980546  
2023-08-31 12:59:23.195: Find a better model.
2023-08-31 13:00:24.919: [iter 6 : loss : 0.4835 = 0.1981 + 0.2837 + 0.0018, time: 61.697102]
2023-08-31 13:00:25.260: epoch 6:	0.13503131  	0.21639270  	0.22672978  
2023-08-31 13:00:25.260: Find a better model.
2023-08-31 13:01:26.237: [iter 7 : loss : 0.4742 = 0.1903 + 0.2820 + 0.0020, time: 60.946504]
2023-08-31 13:01:26.601: epoch 7:	0.13892196  	0.22458716  	0.23336244  
2023-08-31 13:01:26.601: Find a better model.
2023-08-31 13:02:28.143: [iter 8 : loss : 0.4651 = 0.1823 + 0.2806 + 0.0021, time: 61.517676]
2023-08-31 13:02:28.452: epoch 8:	0.14147195  	0.22905664  	0.23819686  
2023-08-31 13:02:28.452: Find a better model.
2023-08-31 13:03:29.243: [iter 9 : loss : 0.4571 = 0.1754 + 0.2794 + 0.0023, time: 60.769318]
2023-08-31 13:03:29.599: epoch 9:	0.14421199  	0.23398526  	0.24327600  
2023-08-31 13:03:29.599: Find a better model.
2023-08-31 13:04:31.108: [iter 10 : loss : 0.4506 = 0.1698 + 0.2784 + 0.0025, time: 61.490307]
2023-08-31 13:04:31.475: epoch 10:	0.14625658  	0.23857082  	0.24713317  
2023-08-31 13:04:31.475: Find a better model.
2023-08-31 13:05:35.261: [iter 11 : loss : 0.4447 = 0.1646 + 0.2775 + 0.0026, time: 63.761811]
2023-08-31 13:05:35.777: epoch 11:	0.14821830  	0.24165219  	0.25075930  
2023-08-31 13:05:35.777: Find a better model.
2023-08-31 13:06:44.293: [iter 12 : loss : 0.4397 = 0.1603 + 0.2767 + 0.0028, time: 68.488562]
2023-08-31 13:06:44.651: epoch 12:	0.14984916  	0.24394679  	0.25303590  
2023-08-31 13:06:44.651: Find a better model.
2023-08-31 13:07:43.167: [iter 13 : loss : 0.4351 = 0.1562 + 0.2759 + 0.0029, time: 58.495155]
2023-08-31 13:07:43.467: epoch 13:	0.15116544  	0.24709313  	0.25583595  
2023-08-31 13:07:43.467: Find a better model.
2023-08-31 13:08:29.459: [iter 14 : loss : 0.4300 = 0.1516 + 0.2752 + 0.0031, time: 45.968009]
2023-08-31 13:08:29.756: epoch 14:	0.15262234  	0.24960749  	0.25857264  
2023-08-31 13:08:29.756: Find a better model.
2023-08-31 13:09:15.466: [iter 15 : loss : 0.4268 = 0.1490 + 0.2746 + 0.0032, time: 45.681808]
2023-08-31 13:09:15.764: epoch 15:	0.15356600  	0.25099105  	0.26034784  
2023-08-31 13:09:15.764: Find a better model.
2023-08-31 13:10:01.950: [iter 16 : loss : 0.4224 = 0.1450 + 0.2740 + 0.0033, time: 46.152670]
2023-08-31 13:10:02.246: epoch 16:	0.15453447  	0.25298202  	0.26211837  
2023-08-31 13:10:02.246: Find a better model.
2023-08-31 13:10:48.237: [iter 17 : loss : 0.4194 = 0.1425 + 0.2735 + 0.0035, time: 45.969765]
2023-08-31 13:10:48.503: epoch 17:	0.15537070  	0.25407770  	0.26348418  
2023-08-31 13:10:48.503: Find a better model.
2023-08-31 13:11:34.636: [iter 18 : loss : 0.4161 = 0.1395 + 0.2730 + 0.0036, time: 46.110065]
2023-08-31 13:11:34.936: epoch 18:	0.15619847  	0.25542936  	0.26541251  
2023-08-31 13:11:34.936: Find a better model.
2023-08-31 13:12:20.914: [iter 19 : loss : 0.4134 = 0.1372 + 0.2725 + 0.0037, time: 45.943119]
2023-08-31 13:12:21.219: epoch 19:	0.15718368  	0.25724036  	0.26703048  
2023-08-31 13:12:21.219: Find a better model.
2023-08-31 13:13:07.088: [iter 20 : loss : 0.4103 = 0.1344 + 0.2721 + 0.0039, time: 45.848515]
2023-08-31 13:13:07.382: epoch 20:	0.15804462  	0.25899476  	0.26820511  
2023-08-31 13:13:07.383: Find a better model.
2023-08-31 13:13:53.247: [iter 21 : loss : 0.4078 = 0.1322 + 0.2717 + 0.0040, time: 45.842512]
2023-08-31 13:13:53.549: epoch 21:	0.15891388  	0.26068413  	0.26963520  
2023-08-31 13:13:53.549: Find a better model.
2023-08-31 13:14:40.198: [iter 22 : loss : 0.4052 = 0.1298 + 0.2713 + 0.0041, time: 46.627390]
2023-08-31 13:14:40.476: epoch 22:	0.15996526  	0.26280227  	0.27091813  
2023-08-31 13:14:40.476: Find a better model.
2023-08-31 13:15:26.590: [iter 23 : loss : 0.4019 = 0.1267 + 0.2710 + 0.0043, time: 46.089319]
2023-08-31 13:15:26.896: epoch 23:	0.16032946  	0.26380292  	0.27202937  
2023-08-31 13:15:26.896: Find a better model.
2023-08-31 13:16:13.045: [iter 24 : loss : 0.4006 = 0.1255 + 0.2706 + 0.0044, time: 46.127760]
2023-08-31 13:16:13.340: epoch 24:	0.16122334  	0.26640284  	0.27372569  
2023-08-31 13:16:13.341: Find a better model.
2023-08-31 13:16:59.379: [iter 25 : loss : 0.3994 = 0.1246 + 0.2703 + 0.0045, time: 46.017744]
2023-08-31 13:16:59.683: epoch 25:	0.16153803  	0.26705921  	0.27406123  
2023-08-31 13:16:59.684: Find a better model.
2023-08-31 13:17:45.348: [iter 26 : loss : 0.3961 = 0.1215 + 0.2699 + 0.0046, time: 45.640248]
2023-08-31 13:17:45.651: epoch 26:	0.16200152  	0.26787078  	0.27554625  
2023-08-31 13:17:45.651: Find a better model.
2023-08-31 13:18:31.395: [iter 27 : loss : 0.3943 = 0.1198 + 0.2697 + 0.0047, time: 45.719159]
2023-08-31 13:18:31.670: epoch 27:	0.16248158  	0.26891154  	0.27631620  
2023-08-31 13:18:31.670: Find a better model.
2023-08-31 13:19:17.700: [iter 28 : loss : 0.3924 = 0.1181 + 0.2694 + 0.0049, time: 46.012995]
2023-08-31 13:19:18.007: epoch 28:	0.16293682  	0.26957291  	0.27750856  
2023-08-31 13:19:18.007: Find a better model.
2023-08-31 13:20:04.278: [iter 29 : loss : 0.3908 = 0.1167 + 0.2692 + 0.0050, time: 46.245152]
2023-08-31 13:20:04.551: epoch 29:	0.16316032  	0.26971242  	0.27736771  
2023-08-31 13:20:04.552: Find a better model.
2023-08-31 13:20:50.301: [iter 30 : loss : 0.3883 = 0.1142 + 0.2689 + 0.0051, time: 45.728179]
2023-08-31 13:20:50.604: epoch 30:	0.16366540  	0.27119631  	0.27864075  
2023-08-31 13:20:50.604: Find a better model.
2023-08-31 13:21:37.434: [iter 31 : loss : 0.3871 = 0.1131 + 0.2687 + 0.0052, time: 46.806749]
2023-08-31 13:21:37.737: epoch 31:	0.16399647  	0.27154222  	0.27848491  
2023-08-31 13:21:37.737: Find a better model.
2023-08-31 13:22:23.679: [iter 32 : loss : 0.3857 = 0.1118 + 0.2686 + 0.0053, time: 45.921924]
2023-08-31 13:22:23.957: epoch 32:	0.16407102  	0.27199471  	0.27878803  
2023-08-31 13:22:23.957: Find a better model.
2023-08-31 13:23:09.760: [iter 33 : loss : 0.3844 = 0.1108 + 0.2682 + 0.0054, time: 45.777495]
2023-08-31 13:23:10.128: epoch 33:	0.16489042  	0.27273238  	0.27940717  
2023-08-31 13:23:10.128: Find a better model.
2023-08-31 13:23:56.136: [iter 34 : loss : 0.3827 = 0.1092 + 0.2681 + 0.0055, time: 45.978381]
2023-08-31 13:23:56.409: epoch 34:	0.16523004  	0.27361292  	0.27985874  
2023-08-31 13:23:56.409: Find a better model.
2023-08-31 13:24:53.179: [iter 35 : loss : 0.3811 = 0.1076 + 0.2679 + 0.0056, time: 56.746856]
2023-08-31 13:24:53.480: epoch 35:	0.16503955  	0.27356225  	0.28027800  
2023-08-31 13:25:39.907: [iter 36 : loss : 0.3805 = 0.1071 + 0.2677 + 0.0057, time: 46.405459]
2023-08-31 13:25:40.212: epoch 36:	0.16530454  	0.27357605  	0.28056666  
2023-08-31 13:26:26.371: [iter 37 : loss : 0.3796 = 0.1063 + 0.2675 + 0.0058, time: 46.136448]
2023-08-31 13:26:26.670: epoch 37:	0.16559418  	0.27444431  	0.28103936  
2023-08-31 13:26:26.670: Find a better model.
2023-08-31 13:27:12.652: [iter 38 : loss : 0.3786 = 0.1054 + 0.2674 + 0.0059, time: 45.965318]
2023-08-31 13:27:12.948: epoch 38:	0.16614883  	0.27483088  	0.28158084  
2023-08-31 13:27:12.948: Find a better model.
2023-08-31 13:27:58.438: [iter 39 : loss : 0.3767 = 0.1035 + 0.2672 + 0.0060, time: 45.467903]
2023-08-31 13:27:58.725: epoch 39:	0.16618201  	0.27500346  	0.28162730  
2023-08-31 13:27:58.725: Find a better model.
2023-08-31 13:28:44.698: [iter 40 : loss : 0.3752 = 0.1022 + 0.2670 + 0.0061, time: 45.948756]
2023-08-31 13:28:44.997: epoch 40:	0.16631465  	0.27507311  	0.28162527  
2023-08-31 13:28:44.997: Find a better model.
2023-08-31 13:29:30.600: [iter 41 : loss : 0.3746 = 0.1015 + 0.2669 + 0.0062, time: 45.584075]
2023-08-31 13:29:30.912: epoch 41:	0.16666219  	0.27641106  	0.28223062  
2023-08-31 13:29:30.913: Find a better model.
2023-08-31 13:30:16.841: [iter 42 : loss : 0.3732 = 0.1003 + 0.2667 + 0.0063, time: 45.907559]
2023-08-31 13:30:17.143: epoch 42:	0.16681950  	0.27654222  	0.28234968  
2023-08-31 13:30:17.143: Find a better model.
2023-08-31 13:31:03.118: [iter 43 : loss : 0.3720 = 0.0991 + 0.2665 + 0.0064, time: 45.953150]
2023-08-31 13:31:03.418: epoch 43:	0.16724168  	0.27728373  	0.28272510  
2023-08-31 13:31:03.418: Find a better model.
2023-08-31 13:31:49.543: [iter 44 : loss : 0.3709 = 0.0979 + 0.2664 + 0.0065, time: 46.099700]
2023-08-31 13:31:49.838: epoch 44:	0.16730785  	0.27724990  	0.28325689  
2023-08-31 13:32:35.930: [iter 45 : loss : 0.3707 = 0.0978 + 0.2664 + 0.0066, time: 46.063230]
2023-08-31 13:32:36.238: epoch 45:	0.16738240  	0.27771673  	0.28332546  
2023-08-31 13:32:36.239: Find a better model.
2023-08-31 13:33:22.484: [iter 46 : loss : 0.3692 = 0.0963 + 0.2661 + 0.0067, time: 46.221440]
2023-08-31 13:33:22.790: epoch 46:	0.16763905  	0.27828923  	0.28394473  
2023-08-31 13:33:22.790: Find a better model.
2023-08-31 13:34:08.691: [iter 47 : loss : 0.3683 = 0.0955 + 0.2661 + 0.0068, time: 45.879728]
2023-08-31 13:34:08.990: epoch 47:	0.16787912  	0.27844715  	0.28420478  
2023-08-31 13:34:08.990: Find a better model.
2023-08-31 13:34:54.621: [iter 48 : loss : 0.3671 = 0.0942 + 0.2660 + 0.0069, time: 45.609076]
2023-08-31 13:34:54.902: epoch 48:	0.16794525  	0.27872780  	0.28459084  
2023-08-31 13:34:54.902: Find a better model.
2023-08-31 13:35:40.323: [iter 49 : loss : 0.3673 = 0.0946 + 0.2658 + 0.0070, time: 45.393449]
2023-08-31 13:35:40.572: epoch 49:	0.16826814  	0.27912793  	0.28499034  
2023-08-31 13:35:40.572: Find a better model.
2023-08-31 13:36:26.327: [iter 50 : loss : 0.3656 = 0.0928 + 0.2657 + 0.0070, time: 45.739495]
2023-08-31 13:36:26.608: epoch 50:	0.16864890  	0.27964967  	0.28547284  
2023-08-31 13:36:26.608: Find a better model.
2023-08-31 13:37:12.151: [iter 51 : loss : 0.3647 = 0.0920 + 0.2656 + 0.0071, time: 45.521686]
2023-08-31 13:37:12.427: epoch 51:	0.16894680  	0.27967778  	0.28558055  
2023-08-31 13:37:12.427: Find a better model.
2023-08-31 13:37:57.958: [iter 52 : loss : 0.3643 = 0.0917 + 0.2654 + 0.0072, time: 45.506402]
2023-08-31 13:37:58.242: epoch 52:	0.16902965  	0.27975884  	0.28579977  
2023-08-31 13:37:58.242: Find a better model.
2023-08-31 13:38:43.285: [iter 53 : loss : 0.3634 = 0.0908 + 0.2653 + 0.0073, time: 45.023339]
2023-08-31 13:38:43.574: epoch 53:	0.16912900  	0.27957982  	0.28527457  
2023-08-31 13:39:28.817: [iter 54 : loss : 0.3624 = 0.0898 + 0.2653 + 0.0074, time: 45.224327]
2023-08-31 13:39:29.099: epoch 54:	0.16917032  	0.27945614  	0.28571194  
2023-08-31 13:40:01.565: [iter 55 : loss : 0.3622 = 0.0895 + 0.2652 + 0.0075, time: 32.441237]
2023-08-31 13:40:01.763: epoch 55:	0.16926964  	0.28084984  	0.28579345  
2023-08-31 13:40:01.763: Find a better model.
2023-08-31 13:40:26.877: [iter 56 : loss : 0.3604 = 0.0878 + 0.2650 + 0.0076, time: 25.095231]
2023-08-31 13:40:27.101: epoch 56:	0.16917849  	0.27996382  	0.28535107  
2023-08-31 13:41:04.280: [iter 57 : loss : 0.3601 = 0.0875 + 0.2650 + 0.0076, time: 37.156059]
2023-08-31 13:41:04.517: epoch 57:	0.16928622  	0.27984387  	0.28535572  
2023-08-31 13:41:31.765: [iter 58 : loss : 0.3594 = 0.0868 + 0.2649 + 0.0077, time: 27.217570]
2023-08-31 13:41:32.001: epoch 58:	0.16940215  	0.27941722  	0.28540963  
2023-08-31 13:41:59.309: [iter 59 : loss : 0.3590 = 0.0864 + 0.2648 + 0.0078, time: 27.282259]
2023-08-31 13:41:59.541: epoch 59:	0.16944364  	0.28009456  	0.28540263  
2023-08-31 13:42:26.612: [iter 60 : loss : 0.3582 = 0.0857 + 0.2646 + 0.0079, time: 27.040317]
2023-08-31 13:42:26.847: epoch 60:	0.16945171  	0.28035322  	0.28561109  
2023-08-31 13:42:54.104: [iter 61 : loss : 0.3579 = 0.0853 + 0.2646 + 0.0080, time: 27.229887]
2023-08-31 13:42:54.334: epoch 61:	0.16948491  	0.27984330  	0.28545514  
2023-08-31 13:43:21.409: [iter 62 : loss : 0.3572 = 0.0847 + 0.2645 + 0.0080, time: 27.051843]
2023-08-31 13:43:21.642: epoch 62:	0.16971664  	0.27995905  	0.28581637  
2023-08-31 13:43:48.457: [iter 63 : loss : 0.3569 = 0.0843 + 0.2645 + 0.0081, time: 26.789703]
2023-08-31 13:43:48.694: epoch 63:	0.16985759  	0.27998483  	0.28574300  
2023-08-31 13:44:15.942: [iter 64 : loss : 0.3561 = 0.0836 + 0.2643 + 0.0082, time: 27.219135]
2023-08-31 13:44:16.179: epoch 64:	0.16997325  	0.28006873  	0.28547928  
2023-08-31 13:44:43.328: [iter 65 : loss : 0.3556 = 0.0831 + 0.2643 + 0.0083, time: 27.124645]
2023-08-31 13:44:43.546: epoch 65:	0.17041193  	0.28073257  	0.28577846  
2023-08-31 13:44:43.546: Early stopping is trigger at epoch: 65
2023-08-31 13:44:43.546: best_result@epoch 55:

2023-08-31 13:44:43.546: 		0.1693      	0.2808      	0.2858      
