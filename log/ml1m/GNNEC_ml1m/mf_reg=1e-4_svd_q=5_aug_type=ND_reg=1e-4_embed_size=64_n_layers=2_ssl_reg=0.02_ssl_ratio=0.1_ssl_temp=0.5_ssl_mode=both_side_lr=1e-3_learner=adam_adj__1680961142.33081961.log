2023-04-08 21:39:02.331: my pid: 20923
2023-04-08 21:39:02.331: model: model.general_recommender.GNNEC_ml1m
2023-04-08 21:39:02.331: Dataset statistics:
Name: ml1m
The number of users: 6040
The number of items: 3706
The number of ratings: 1000209
Average actions of users: 165.60
Average actions of items: 269.89
The sparsity of the dataset: 95.531637%

The number of training: 902826
The number of validation: 0
The number of testing: 97383
2023-04-08 21:39:02.331: NeuRec:[NeuRec]:
recommender=GNNEC_ml1m
dataset=ml1m
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

GNNEC_ml1m:[hyperparameters]:
mf_reg=1e-4
svd_q=5
aug_type=ND
reg=1e-4
embed_size=64
n_layers=2
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
ssl_mode=both_side
lr=1e-3
learner=adam
adj_type=pre
epochs=100
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=10
pretrain_flag=0
save_flag=0

Command line:
recommender=GNNEC_ml1m
dataset=ml1m
epochs=100
n_layers=2
embed_size=64
batch_size=2048
lr=1e-3
aug_type=ND
reg=1e-4
ssl_reg=0.02
ssl_ratio=0.1
ssl_temp=0.5
mf_reg=1e-4
svd_q=5
2023-04-08 21:39:08.713: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-04-08 21:40:13.670: [iter 1 : loss : 0.9417 = 0.6831 + 0.2586 + 0.0000, time: 64.955447]
2023-04-08 21:40:14.023: epoch 1:	0.09412146  	0.15075016  	0.16199093  
2023-04-08 21:40:14.023: Find a better model.
2023-04-08 21:40:57.492: [iter 2 : loss : 0.6527 = 0.3526 + 0.2995 + 0.0006, time: 43.447274]
2023-04-08 21:40:57.684: epoch 2:	0.09904655  	0.15413888  	0.16569519  
2023-04-08 21:40:57.684: Find a better model.
2023-04-08 21:41:41.576: [iter 3 : loss : 0.5535 = 0.2582 + 0.2942 + 0.0010, time: 43.870903]
2023-04-08 21:41:41.790: epoch 3:	0.11516373  	0.18219653  	0.19354820  
2023-04-08 21:41:41.790: Find a better model.
2023-04-08 21:42:35.242: [iter 4 : loss : 0.5186 = 0.2285 + 0.2888 + 0.0013, time: 53.427997]
2023-04-08 21:42:35.593: epoch 4:	0.12436891  	0.19823363  	0.20908035  
2023-04-08 21:42:35.593: Find a better model.
2023-04-08 21:43:40.409: [iter 5 : loss : 0.4980 = 0.2107 + 0.2857 + 0.0015, time: 64.793545]
2023-04-08 21:43:40.763: epoch 5:	0.13080938  	0.20980591  	0.21980502  
2023-04-08 21:43:40.763: Find a better model.
2023-04-08 21:44:45.368: [iter 6 : loss : 0.4835 = 0.1981 + 0.2837 + 0.0018, time: 64.584588]
2023-04-08 21:44:45.722: epoch 6:	0.13502304  	0.21638440  	0.22672507  
2023-04-08 21:44:45.722: Find a better model.
2023-04-08 21:45:37.856: [iter 7 : loss : 0.4742 = 0.1903 + 0.2820 + 0.0020, time: 52.114680]
2023-04-08 21:45:38.164: epoch 7:	0.13893852  	0.22459145  	0.23337178  
2023-04-08 21:45:38.164: Find a better model.
2023-04-08 21:46:21.469: [iter 8 : loss : 0.4651 = 0.1823 + 0.2806 + 0.0021, time: 43.281110]
2023-04-08 21:46:21.766: epoch 8:	0.14147195  	0.22905664  	0.23819195  
2023-04-08 21:46:21.766: Find a better model.
2023-04-08 21:47:05.675: [iter 9 : loss : 0.4571 = 0.1754 + 0.2794 + 0.0023, time: 43.884394]
2023-04-08 21:47:06.023: epoch 9:	0.14421199  	0.23398526  	0.24327405  
2023-04-08 21:47:06.023: Find a better model.
2023-04-08 21:48:10.273: [iter 10 : loss : 0.4506 = 0.1698 + 0.2784 + 0.0025, time: 64.233681]
2023-04-08 21:48:10.617: epoch 10:	0.14624831  	0.23856765  	0.24712263  
2023-04-08 21:48:10.617: Find a better model.
2023-04-08 21:49:15.429: [iter 11 : loss : 0.4447 = 0.1646 + 0.2775 + 0.0026, time: 64.792823]
2023-04-08 21:49:15.772: epoch 11:	0.14821002  	0.24163713  	0.25075191  
2023-04-08 21:49:15.772: Find a better model.
2023-04-08 21:50:14.853: [iter 12 : loss : 0.4397 = 0.1603 + 0.2767 + 0.0028, time: 59.061252]
2023-04-08 21:50:15.147: epoch 12:	0.14984916  	0.24395230  	0.25304836  
2023-04-08 21:50:15.148: Find a better model.
2023-04-08 21:50:57.995: [iter 13 : loss : 0.4351 = 0.1562 + 0.2759 + 0.0029, time: 42.823796]
2023-04-08 21:50:58.284: epoch 13:	0.15116544  	0.24709313  	0.25584796  
2023-04-08 21:50:58.284: Find a better model.
2023-04-08 21:51:35.994: [iter 14 : loss : 0.4300 = 0.1516 + 0.2752 + 0.0031, time: 37.687999]
2023-04-08 21:51:36.217: epoch 14:	0.15261407  	0.24959244  	0.25856659  
2023-04-08 21:51:36.217: Find a better model.
2023-04-08 21:52:40.860: [iter 15 : loss : 0.4268 = 0.1490 + 0.2746 + 0.0032, time: 64.620238]
2023-04-08 21:52:41.192: epoch 15:	0.15355773  	0.25098601  	0.26034102  
2023-04-08 21:52:41.193: Find a better model.
2023-04-08 21:53:45.587: [iter 16 : loss : 0.4224 = 0.1450 + 0.2740 + 0.0033, time: 64.377019]
2023-04-08 21:53:45.933: epoch 16:	0.15453447  	0.25297540  	0.26212797  
2023-04-08 21:53:45.934: Find a better model.
2023-04-08 21:54:50.508: [iter 17 : loss : 0.4194 = 0.1425 + 0.2735 + 0.0035, time: 64.551237]
2023-04-08 21:54:50.837: epoch 17:	0.15537898  	0.25411081  	0.26349744  
2023-04-08 21:54:50.837: Find a better model.
2023-04-08 21:55:36.438: [iter 18 : loss : 0.4161 = 0.1395 + 0.2730 + 0.0036, time: 45.580358]
2023-04-08 21:55:36.721: epoch 18:	0.15619020  	0.25542796  	0.26540586  
2023-04-08 21:55:36.721: Find a better model.
2023-04-08 21:56:20.990: [iter 19 : loss : 0.4134 = 0.1372 + 0.2725 + 0.0037, time: 44.247249]
2023-04-08 21:56:21.275: epoch 19:	0.15718368  	0.25724036  	0.26702964  
2023-04-08 21:56:21.275: Find a better model.
2023-04-08 21:57:13.089: [iter 20 : loss : 0.4103 = 0.1344 + 0.2721 + 0.0039, time: 51.792201]
2023-04-08 21:57:13.428: epoch 20:	0.15805291  	0.25899902  	0.26821640  
2023-04-08 21:57:13.428: Find a better model.
2023-04-08 21:58:17.918: [iter 21 : loss : 0.4078 = 0.1322 + 0.2717 + 0.0040, time: 64.470022]
2023-04-08 21:58:18.270: epoch 21:	0.15891388  	0.26068413  	0.26963961  
2023-04-08 21:58:18.270: Find a better model.
2023-04-08 21:59:23.377: [iter 22 : loss : 0.4052 = 0.1298 + 0.2713 + 0.0041, time: 65.084262]
2023-04-08 21:59:23.718: epoch 22:	0.15996526  	0.26279485  	0.27091795  
2023-04-08 21:59:23.718: Find a better model.
2023-04-08 22:00:16.349: [iter 23 : loss : 0.4019 = 0.1267 + 0.2710 + 0.0043, time: 52.610038]
2023-04-08 22:00:16.591: epoch 23:	0.16032119  	0.26385814  	0.27205262  
2023-04-08 22:00:16.591: Find a better model.
2023-04-08 22:00:59.704: [iter 24 : loss : 0.4006 = 0.1255 + 0.2706 + 0.0044, time: 43.094411]
2023-04-08 22:00:59.998: epoch 24:	0.16122334  	0.26648089  	0.27374351  
2023-04-08 22:00:59.998: Find a better model.
2023-04-08 22:01:42.159: [iter 25 : loss : 0.3994 = 0.1246 + 0.2703 + 0.0045, time: 42.136283]
2023-04-08 22:01:42.502: epoch 25:	0.16153803  	0.26705921  	0.27405769  
2023-04-08 22:01:42.502: Find a better model.
2023-04-08 22:02:47.062: [iter 26 : loss : 0.3961 = 0.1215 + 0.2699 + 0.0046, time: 64.541304]
2023-04-08 22:02:47.412: epoch 26:	0.16198497  	0.26784843  	0.27553424  
2023-04-08 22:02:47.413: Find a better model.
2023-04-08 22:03:52.151: [iter 27 : loss : 0.3943 = 0.1198 + 0.2697 + 0.0047, time: 64.717979]
2023-04-08 22:03:52.492: epoch 27:	0.16248158  	0.26891154  	0.27630091  
2023-04-08 22:03:52.492: Find a better model.
2023-04-08 22:04:53.222: [iter 28 : loss : 0.3924 = 0.1181 + 0.2694 + 0.0049, time: 60.710689]
2023-04-08 22:04:53.508: epoch 28:	0.16294509  	0.26957980  	0.27752310  
2023-04-08 22:04:53.508: Find a better model.
2023-04-08 22:05:36.602: [iter 29 : loss : 0.3908 = 0.1167 + 0.2692 + 0.0050, time: 43.071063]
2023-04-08 22:05:36.870: epoch 29:	0.16316032  	0.26971242  	0.27737436  
2023-04-08 22:05:36.870: Find a better model.
2023-04-08 22:06:16.157: [iter 30 : loss : 0.3883 = 0.1142 + 0.2689 + 0.0051, time: 39.270405]
2023-04-08 22:06:16.378: epoch 30:	0.16366540  	0.27117145  	0.27864534  
2023-04-08 22:06:16.379: Find a better model.
2023-04-08 22:07:21.058: [iter 31 : loss : 0.3871 = 0.1131 + 0.2687 + 0.0052, time: 64.655221]
2023-04-08 22:07:21.408: epoch 31:	0.16397992  	0.27147475  	0.27845868  
2023-04-08 22:07:21.408: Find a better model.
2023-04-08 22:08:26.311: [iter 32 : loss : 0.3857 = 0.1118 + 0.2686 + 0.0053, time: 64.880847]
2023-04-08 22:08:26.642: epoch 32:	0.16407102  	0.27198666  	0.27878422  
2023-04-08 22:08:26.643: Find a better model.
2023-04-08 22:09:31.472: [iter 33 : loss : 0.3844 = 0.1108 + 0.2682 + 0.0054, time: 64.810805]
2023-04-08 22:09:31.826: epoch 33:	0.16489042  	0.27274913  	0.27941018  
2023-04-08 22:09:31.826: Find a better model.
2023-04-08 22:10:18.332: [iter 34 : loss : 0.3827 = 0.1092 + 0.2681 + 0.0055, time: 46.484694]
2023-04-08 22:10:18.617: epoch 34:	0.16522175  	0.27361315  	0.27985519  
2023-04-08 22:10:18.617: Find a better model.
2023-04-08 22:11:03.044: [iter 35 : loss : 0.3811 = 0.1076 + 0.2679 + 0.0056, time: 44.405669]
2023-04-08 22:11:03.331: epoch 35:	0.16503128  	0.27355507  	0.28026852  
2023-04-08 22:11:54.384: [iter 36 : loss : 0.3805 = 0.1071 + 0.2677 + 0.0057, time: 51.029185]
2023-04-08 22:11:54.741: epoch 36:	0.16530454  	0.27357605  	0.28055486  
2023-04-08 22:12:59.511: [iter 37 : loss : 0.3796 = 0.1063 + 0.2675 + 0.0058, time: 64.749359]
2023-04-08 22:12:59.855: epoch 37:	0.16556934  	0.27443430  	0.28103673  
2023-04-08 22:12:59.855: Find a better model.
2023-04-08 22:14:04.656: [iter 38 : loss : 0.3786 = 0.1054 + 0.2674 + 0.0059, time: 64.782518]
2023-04-08 22:14:05.015: epoch 38:	0.16614883  	0.27483088  	0.28159577  
2023-04-08 22:14:05.015: Find a better model.
2023-04-08 22:14:58.242: [iter 39 : loss : 0.3767 = 0.1035 + 0.2672 + 0.0060, time: 53.201705]
2023-04-08 22:14:58.532: epoch 39:	0.16615717  	0.27495211  	0.28160810  
2023-04-08 22:14:58.532: Find a better model.
2023-04-08 22:15:41.548: [iter 40 : loss : 0.3752 = 0.1022 + 0.2670 + 0.0061, time: 42.994631]
2023-04-08 22:15:41.832: epoch 40:	0.16630638  	0.27499157  	0.28159329  
2023-04-08 22:15:41.832: Find a better model.
2023-04-08 22:16:24.027: [iter 41 : loss : 0.3746 = 0.1015 + 0.2669 + 0.0062, time: 42.172494]
2023-04-08 22:16:24.359: epoch 41:	0.16664562  	0.27636135  	0.28220356  
2023-04-08 22:16:24.360: Find a better model.
2023-04-08 22:17:29.218: [iter 42 : loss : 0.3732 = 0.1003 + 0.2667 + 0.0063, time: 64.839574]
2023-04-08 22:17:29.559: epoch 42:	0.16681121  	0.27650970  	0.28233486  
2023-04-08 22:17:29.559: Find a better model.
2023-04-08 22:18:34.284: [iter 43 : loss : 0.3720 = 0.0991 + 0.2665 + 0.0064, time: 64.705329]
2023-04-08 22:18:34.631: epoch 43:	0.16723341  	0.27727622  	0.28271663  
2023-04-08 22:18:34.632: Find a better model.
2023-04-08 22:19:35.269: [iter 44 : loss : 0.3708 = 0.0979 + 0.2664 + 0.0065, time: 60.618807]
2023-04-08 22:19:35.563: epoch 44:	0.16730785  	0.27725667  	0.28325292  
2023-04-08 22:20:18.588: [iter 45 : loss : 0.3707 = 0.0978 + 0.2664 + 0.0066, time: 42.995202]
2023-04-08 22:20:18.878: epoch 45:	0.16737412  	0.27773365  	0.28332880  
2023-04-08 22:20:18.878: Find a better model.
2023-04-08 22:20:58.464: [iter 46 : loss : 0.3692 = 0.0963 + 0.2661 + 0.0067, time: 39.567686]
2023-04-08 22:20:58.688: epoch 46:	0.16764733  	0.27832234  	0.28395826  
2023-04-08 22:20:58.688: Find a better model.
2023-04-08 22:22:03.458: [iter 47 : loss : 0.3683 = 0.0955 + 0.2661 + 0.0068, time: 64.744954]
2023-04-08 22:22:03.794: epoch 47:	0.16788739  	0.27842534  	0.28420514  
2023-04-08 22:22:03.794: Find a better model.
2023-04-08 22:23:08.430: [iter 48 : loss : 0.3671 = 0.0942 + 0.2660 + 0.0069, time: 64.617013]
2023-04-08 22:23:08.777: epoch 48:	0.16793698  	0.27871123  	0.28458220  
2023-04-08 22:23:08.777: Find a better model.
2023-04-08 22:24:12.930: [iter 49 : loss : 0.3673 = 0.0946 + 0.2658 + 0.0070, time: 64.134463]
2023-04-08 22:24:13.285: epoch 49:	0.16826816  	0.27908760  	0.28500518  
2023-04-08 22:24:13.286: Find a better model.
2023-04-08 22:24:59.959: [iter 50 : loss : 0.3656 = 0.0928 + 0.2657 + 0.0070, time: 46.655932]
2023-04-08 22:25:00.242: epoch 50:	0.16864063  	0.27963251  	0.28547710  
2023-04-08 22:25:00.242: Find a better model.
2023-04-08 22:25:43.407: [iter 51 : loss : 0.3647 = 0.0920 + 0.2656 + 0.0071, time: 43.144284]
2023-04-08 22:25:43.705: epoch 51:	0.16894679  	0.27965540  	0.28558353  
2023-04-08 22:25:43.705: Find a better model.
2023-04-08 22:26:32.699: [iter 52 : loss : 0.3643 = 0.0917 + 0.2654 + 0.0072, time: 48.969239]
2023-04-08 22:26:33.029: epoch 52:	0.16900481  	0.27971920  	0.28577772  
2023-04-08 22:26:33.030: Find a better model.
2023-04-08 22:27:37.547: [iter 53 : loss : 0.3634 = 0.0908 + 0.2653 + 0.0073, time: 64.495941]
2023-04-08 22:27:37.899: epoch 53:	0.16914557  	0.27959797  	0.28528488  
2023-04-08 22:28:42.886: [iter 54 : loss : 0.3624 = 0.0898 + 0.2653 + 0.0074, time: 64.969123]
2023-04-08 22:28:43.228: epoch 54:	0.16914546  	0.27938962  	0.28567860  
2023-04-08 22:29:37.472: [iter 55 : loss : 0.3622 = 0.0895 + 0.2652 + 0.0075, time: 54.223438]
2023-04-08 22:29:37.760: epoch 55:	0.16926135  	0.28082114  	0.28577501  
2023-04-08 22:29:37.761: Find a better model.
2023-04-08 22:30:20.960: [iter 56 : loss : 0.3604 = 0.0878 + 0.2650 + 0.0076, time: 43.178821]
2023-04-08 22:30:21.251: epoch 56:	0.16915368  	0.27987963  	0.28532931  
2023-04-08 22:31:02.804: [iter 57 : loss : 0.3601 = 0.0875 + 0.2650 + 0.0076, time: 41.530731]
2023-04-08 22:31:03.134: epoch 57:	0.16930276  	0.27991125  	0.28536347  
2023-04-08 22:32:07.829: [iter 58 : loss : 0.3594 = 0.0868 + 0.2649 + 0.0077, time: 64.677335]
2023-04-08 22:32:08.176: epoch 58:	0.16938558  	0.27934271  	0.28538445  
2023-04-08 22:33:13.032: [iter 59 : loss : 0.3590 = 0.0864 + 0.2648 + 0.0078, time: 64.837715]
2023-04-08 22:33:13.381: epoch 59:	0.16946019  	0.28019390  	0.28543359  
2023-04-08 22:34:14.669: [iter 60 : loss : 0.3582 = 0.0857 + 0.2646 + 0.0079, time: 61.267325]
2023-04-08 22:34:14.956: epoch 60:	0.16945998  	0.28035718  	0.28562954  
2023-04-08 22:34:57.773: [iter 61 : loss : 0.3579 = 0.0853 + 0.2646 + 0.0080, time: 42.796681]
2023-04-08 22:34:58.060: epoch 61:	0.16946836  	0.27979410  	0.28543732  
2023-04-08 22:35:38.283: [iter 62 : loss : 0.3572 = 0.0847 + 0.2645 + 0.0080, time: 40.201747]
2023-04-08 22:35:38.506: epoch 62:	0.16972491  	0.27994731  	0.28580955  
2023-04-08 22:36:41.173: [iter 63 : loss : 0.3569 = 0.0843 + 0.2645 + 0.0081, time: 62.644538]
2023-04-08 22:36:41.525: epoch 63:	0.16984932  	0.27999535  	0.28573906  
2023-04-08 22:37:46.117: [iter 64 : loss : 0.3561 = 0.0836 + 0.2643 + 0.0082, time: 64.570173]
2023-04-08 22:37:46.462: epoch 64:	0.16996498  	0.28006777  	0.28547460  
2023-04-08 22:38:50.670: [iter 65 : loss : 0.3556 = 0.0831 + 0.2643 + 0.0083, time: 64.190037]
2023-04-08 22:38:51.011: epoch 65:	0.17041193  	0.28074461  	0.28579167  
2023-04-08 22:38:51.011: Early stopping is trigger at epoch: 65
2023-04-08 22:38:51.011: best_result@epoch 55:

2023-04-08 22:38:51.011: 		0.1693      	0.2808      	0.2858      
